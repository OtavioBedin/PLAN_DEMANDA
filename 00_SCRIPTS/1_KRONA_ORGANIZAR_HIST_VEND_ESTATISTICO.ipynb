{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa7d6663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Mapeamento de pastas concluÃ­do com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Importando bibliotecas\n",
    "from functions import *\n",
    "import pandas as pd\n",
    "import locale\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import duckdb\n",
    "import gc\n",
    "import numpy as np\n",
    "import warnings\n",
    "import logging\n",
    "import shutil\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING, format='%(message)s')\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "timer = Temporizador()\n",
    "timer.iniciar()\n",
    "\n",
    "locale.setlocale(locale.LC_TIME, 'Portuguese_Brazil.1252')  # Para Windows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "# Detecta se o script estÃ¡ sendo executado de um .py ou de um notebook\n",
    "try:\n",
    "    caminho_base = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    # __file__ nÃ£o existe em Jupyter ou ambiente interativo\n",
    "    caminho_base = Path.cwd()\n",
    "\n",
    "pasta_input_parquet = caminho_base.parent / '01_INPUT_PIPELINE/01_BD_PARQUET'\n",
    "arquivo_input_regras_negocio = caminho_base.parent / '01_INPUT_PIPELINE/02_REGRAS_NEGOCIO/KRONA_REGRAS.xlsm'\n",
    "pasta_staging_parquet = caminho_base.parent / '02_STAGING_PARQUET' # Armazena arquivos parquet com tratamentos, aplicaÃ§Ãµes de regras, depara, etc\n",
    "pasta_input_painel = caminho_base.parent / '03_INPUT_PAINEL' # Armazena arquivos que serÃ£o consumidos no painel de S&OP para os gerentes\n",
    "pasta_painel = caminho_base.parent / '05_PAINEL'\n",
    "\n",
    "# # Eliminar arquivos das pastas de 02_STAGING_PARQUET e 03_INPUT_PAINEL que serÃ£o regenerados\n",
    "# pastas_para_limpar = [\n",
    "#     pasta_staging_parquet,\n",
    "#     pasta_input_painel,\n",
    "# ]\n",
    "\n",
    "# for pasta in pastas_para_limpar:\n",
    "#     if pasta.exists() and pasta.is_dir():\n",
    "#         for item in pasta.iterdir():\n",
    "#             if item.is_file() or item.is_symlink():\n",
    "#                 item.unlink()\n",
    "#             elif item.is_dir():\n",
    "#                 shutil.rmtree(item)\n",
    "\n",
    "print(\"âœ… Mapeamento de pastas concluÃ­do com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cf1f53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ImportaÃ§Ã£o e tratamento de dados do arquivo KRONA_REGRAS, concluÃ­dos com sucesso!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Carregar dados arquivo KRONA_REGRAS\n",
    "caminho_arquivo = arquivo_input_regras_negocio\n",
    "\n",
    "#-----------------------------------------------------------------------#\n",
    "#--------------- Carregar produtos eliminar ----------------------------#\n",
    "#-----------------------------------------------------------------------#\n",
    "guia_excel = 'PRODUTOS_ELIMINAR'\n",
    "df_produtos_eliminar = pd.read_excel(caminho_arquivo, sheet_name=guia_excel, engine='calamine', dtype={'COD_PROD': str})\n",
    "df_produtos_eliminar['COD_PROD'] = df_produtos_eliminar['COD_PROD'].astype(str)\n",
    "df_produtos_eliminar = df_produtos_eliminar.drop_duplicates(subset=['COD_PROD'])\n",
    "df_produtos_eliminar = df_produtos_eliminar[df_produtos_eliminar['COD_PROD'].notna()].reset_index(drop=True)\n",
    "\n",
    "#-----------------------------------------------------------------------#\n",
    "#---------------Carregar Regionais Gestor ------------------------------#\n",
    "#-----------------------------------------------------------------------#\n",
    "guia_excel = 'REGIONAIS_GESTOR'\n",
    "df_regionais_gestor = pd.read_excel(caminho_arquivo, sheet_name=guia_excel, engine='calamine')\n",
    "df_regionais_gestor = df_regionais_gestor.drop_duplicates(subset=['REGIONAL', 'REGIONAL_GESTOR'])\n",
    "df_regionais_gestor = df_regionais_gestor[df_regionais_gestor['REGIONAL'].notna()].reset_index(drop=True)\n",
    "\n",
    "#-----------------------------------------------------------------------#\n",
    "#---------------Carrregar Demanda LanÃ§amento Novos Produtos ------------#\n",
    "#-----------------------------------------------------------------------#\n",
    "guia_excel = 'PRODUTOS_LANCAMENTOS'\n",
    "df_produtos_lancamento = pd.read_excel(caminho_arquivo, sheet_name=guia_excel, engine='calamine')\n",
    "# ðŸš¨ VALIDAR SE EXISTEM DADOS\n",
    "if df_produtos_lancamento.empty:\n",
    "    raise ValueError(\n",
    "        \"âŒ ERRO: Nenhuma informaÃ§Ã£o foi encontrada na aba PRODUTOS_LANCAMENTOS.\\n\"\n",
    "        \"âž¡ï¸ Verifique se a planilha possui dados vÃ¡lidos antes de executar o pipeline.\"\n",
    "    )\n",
    "df_produtos_lancamento['JANELA LANÃ‡AMENTO'] = df_produtos_lancamento['JANELA LANÃ‡AMENTO'].astype(str).str.strip()\n",
    "df_produtos_lancamento = df_produtos_lancamento[df_produtos_lancamento['JANELA LANÃ‡AMENTO'] != ''].reset_index(drop=True)\n",
    "df_produtos_lancamento.rename(columns={'COD': 'COD_PROD'}, inplace=True)\n",
    "df_produtos_lancamento = df_produtos_lancamento[df_produtos_lancamento['COD_PROD'].notna()].reset_index(drop=True)\n",
    "df_produtos_lancamento['COD_PROD'] = df_produtos_lancamento['COD_PROD'].astype(str)\n",
    "\n",
    "# Identifica colunas com datas vÃ¡lidas\n",
    "col_datas = []\n",
    "for col in df_produtos_lancamento.columns:\n",
    "    try:\n",
    "        pd.to_datetime(col, dayfirst=True, errors='raise')\n",
    "        col_datas.append(col)\n",
    "    except (ValueError, TypeError):\n",
    "        continue\n",
    "\n",
    "colunas_validas = ['COD_PROD'] + \\\n",
    "                    [col for col in df_produtos_lancamento.columns if 'CD:' in str(col)] + \\\n",
    "                    col_datas\n",
    "df_produtos_lancamento = df_produtos_lancamento[[col for col in colunas_validas if col in df_produtos_lancamento.columns]]\n",
    "\n",
    "# Transforma datas em linhas\n",
    "df_produtos_lancamento = df_produtos_lancamento.melt(\n",
    "    id_vars=[col for col in df_produtos_lancamento.columns if col not in col_datas],\n",
    "    value_vars=col_datas,\n",
    "    var_name='PERIODO',\n",
    "    value_name='VALOR'\n",
    ")\n",
    "df_produtos_lancamento = df_produtos_lancamento[df_produtos_lancamento['VALOR'].notna()].reset_index(drop=True)\n",
    "\n",
    "# Multiplica colunas CD pelo VALOR\n",
    "colunas_cd = [col for col in df_produtos_lancamento.columns if 'CD:' in str(col)]\n",
    "for col in colunas_cd:\n",
    "    df_produtos_lancamento[col] = df_produtos_lancamento[col] * df_produtos_lancamento['VALOR']\n",
    "df_produtos_lancamento.drop(columns=['VALOR'], inplace=True)\n",
    "\n",
    "# Transforma colunas CD em linhas\n",
    "df_produtos_lancamento = df_produtos_lancamento.melt(\n",
    "    id_vars=[col for col in df_produtos_lancamento.columns if col not in colunas_cd],\n",
    "    value_vars=colunas_cd,\n",
    "    var_name='CD',\n",
    "    value_name='QTD'\n",
    ")\n",
    "\n",
    "# Carregar a tabela DE_PARA_CD para fazer de para das unidades CD\n",
    "guia_excel = 'DE_PARA_CD'\n",
    "df_de_para_cd = pd.read_excel(caminho_arquivo, sheet_name=guia_excel, engine='calamine', dtype={'DE': str, 'PARA': str})\n",
    "\n",
    "# Fazer merge do df_produtos_lancamento com o df_de_para_cd para substituir os cÃ³digos CD\n",
    "df_produtos_lancamento = df_produtos_lancamento.merge(\n",
    "    df_de_para_cd,\n",
    "    left_on='CD',\n",
    "    right_on='DE',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Atualizar a coluna CD com os valores da coluna PARA\n",
    "df_produtos_lancamento['CD'] = df_produtos_lancamento['PARA']\n",
    "\n",
    "# Remover colunas DE e PARA\n",
    "df_produtos_lancamento.drop(columns=['DE', 'PARA'], inplace=True)\n",
    "\n",
    "# Coluna CD transformar em maiusculo e remover espaÃ§os em branco\n",
    "df_produtos_lancamento['CD'] = df_produtos_lancamento['CD'].astype(str).str.upper().str.strip()\n",
    "\n",
    "# Salvar em parquet para uso posterior\n",
    "df_produtos_lancamento.to_parquet(pasta_staging_parquet / 'DEMANDA_LANCAMENTO_PRODUTOS_KRONA.parquet', index=False)\n",
    "\n",
    "#-----------------------------------------------------------------------#\n",
    "#---------------Carregar Regionais Construtora -------------------------#\n",
    "#-----------------------------------------------------------------------#\n",
    "guia_excel = 'REGIONAIS_CONSTRUTORA'\n",
    "df_regionais_construtora = pd.read_excel(caminho_arquivo, sheet_name=guia_excel, engine='calamine')\n",
    "df_regionais_construtora = df_regionais_construtora.drop_duplicates(subset=['REGIONAL BASE', 'REGIONAL ATUALIZADA'])\n",
    "\n",
    "#-----------------------------------------------------------------------#\n",
    "#---------------Carregar Clientes para planejamento de Demanda----------#\n",
    "#-----------------------------------------------------------------------#\n",
    "guia_excel = 'CLIENTES_DEMANDA'\n",
    "df_clientes_plan_demanda = pd.read_excel(caminho_arquivo, sheet_name=guia_excel, engine='calamine', dtype={'Cod_Grupo_Cliente': str})\n",
    "df_clientes_plan_demanda = df_clientes_plan_demanda.drop_duplicates(subset=['Cod_Grupo_Cliente'])\n",
    "df_clientes_plan_demanda = df_clientes_plan_demanda[df_clientes_plan_demanda['Cod_Grupo_Cliente'].notna()].reset_index(drop=True)\n",
    "\n",
    "# Converter a coluna de clientes para set para acelerar o isin\n",
    "lista_clientes_plan_demanda = set(df_clientes_plan_demanda['Cod_Grupo_Cliente'])\n",
    "\n",
    "# Unir COD_PROD de df_produtos_lancamento e df_produtos_eliminar, formar uma unica lista de produtos a eliminar, e remover do df_fato_vendas_krona\n",
    "# produtos_a_eliminar = pd.concat([df_produtos_eliminar[['COD_PROD']], df_produtos_lancamento[['COD_PROD']]]).drop_duplicates().reset_index(drop=True)\n",
    "# FIXME: Retirei os produtos de lanÃ§amento da lista de exclusÃ£o conforme solicitaÃ§Ã£o da Anna no WORD\n",
    "produtos_a_eliminar = df_produtos_eliminar[['COD_PROD']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "#-----------------------------------------------------------------------#\n",
    "#---------------Carregar DIRECIONA_CLIENTES_REGIONAL--------------------#\n",
    "#-----------------------------------------------------------------------#\n",
    "guia_excel = 'DIRECIONA_CLIENTES_REGIONAL'\n",
    "df_direc_cli_regional = pd.read_excel(caminho_arquivo, sheet_name=guia_excel, engine='calamine', dtype={'COD_GRUPO_CLIENTE': str, 'COD_CLIENTE': str})\n",
    "df_direc_cli_regional = df_direc_cli_regional[df_direc_cli_regional['COD_CLIENTE'].notna()].reset_index(drop=True)\n",
    "\n",
    "#-----------------------------------------------------------------------#\n",
    "#---------------Carregar PERIODO_PREVISAO-------------------------------#\n",
    "#-----------------------------------------------------------------------#\n",
    "guia_excel = 'PERIODO_PREVISAO'\n",
    "df_periodo_previsao = pd.read_excel(caminho_arquivo, sheet_name=guia_excel, engine='calamine')\n",
    "df_periodo_previsao = df_periodo_previsao[df_periodo_previsao['PERIODO_PROJECAO'].notna()].reset_index(drop=True)\n",
    "df_periodo_previsao = df_periodo_previsao.drop_duplicates(subset=['PERIODO_PROJECAO'])\n",
    "\n",
    "print(\"âœ… ImportaÃ§Ã£o e tratamento de dados do arquivo KRONA_REGRAS, concluÃ­dos com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54aed134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Script para eliminar duplicaÃ§Ã£o de Chv_Cliente no Dim_Clientes_Krona, conforme orientado por Marcos TI, criamos essa rotina para encontrar as duplicaÃ§Ãµes, eliminar e gerar novo Parquet sem duplicaÃ§Ãµes.\n",
    "\n",
    "# Carregar o Parquet\n",
    "df_dim_cli_krona = pd.read_parquet(pasta_input_parquet / \"Dim_Clientes_Krona.parquet\")\n",
    "\n",
    "# Eliminar duplciaÃ§Ãµes mantendo a primeira ocorrÃªncia\n",
    "df_dim_cli_krona = df_dim_cli_krona.drop_duplicates(subset=[\"Chv_Cliente\"], keep='first').reset_index(drop=True)\n",
    "\n",
    "# Gerar novo Parquet sem duplicaÃ§Ãµes\n",
    "df_dim_cli_krona.to_parquet(pasta_input_parquet / \"Dim_Clientes_Krona.parquet\", index=False)\n",
    "\n",
    "del df_dim_cli_krona\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8df03eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b379d6fe74f4807b49167bcddd96628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Carregamento de df_vendas_krona_silver concluÃ­do com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Where Des_Origem = 'Krona'\n",
    "#   And Cod_Empresa IN ('01','05','08','0802','10')\n",
    "#   And Dat_Emissao_Venda >=Â '01/01/2018';\n",
    "\n",
    "# Na dimensÃ£o de Produto tem o seguinte filtro:\n",
    "# Where Cod_Empresa IN ('01','05','08','0802','10')\n",
    "\n",
    "# Nos casos de pedidos cancelados, consideramos apenas o que foi realmente vendido ao cliente:\n",
    "# Se o pedido foi cancelado por completo, nÃ£o aparece nenhuma venda.\n",
    "# Se apenas uma parte foi cancelada, consideramos somente a parte que foi vendida.\n",
    "# AlÃ©m disso, tambÃ©m existem os bloqueios de pedidos, que representam a â€œetapaâ€ em que o pedido se encontra. Nesses casos, Ã© importante definir quais bloqueios devem ser considerados nessa anÃ¡lise.\n",
    "\n",
    "# falei com o AndrÃ© aqui pelo chat, e verificou comÂ aÂ Aline\n",
    "# a principio utiliza os cÃ³digos de bloqueio 80, 90Â ,Â 95Â eÂ 99\n",
    "\n",
    "# No arquivo parquet, precisamos filtrar o campo Des_Origem = \"Krona\". O motivo Ã© que existem pedidos faturados pelo Protheus que contam no resultado da Viqua, e este aplicativo \"CML - Vendas\" do Qlik Sense traz apenas os pedidos que geram resultadoÂ paraÂ aÂ Krona\n",
    "\n",
    "# Consolidando as informaÃ§Ãµes de fontes em Parquet\n",
    "# Conslidando as informaÃ§Ãµes de fontes em Parquet\n",
    "empresa = 'Krona'\n",
    "fact = (pasta_input_parquet / \"Fato_Vendas_Krona.parquet\").as_posix()\n",
    "prod = (pasta_input_parquet / \"Dim_Produtos_Vendas_Krona.parquet\").as_posix()\n",
    "cli  = (pasta_input_parquet / \"Dim_Clientes_Krona.parquet\").as_posix()\n",
    "vend = (pasta_input_parquet / \"Dim_Vendedores_Krona.parquet\").as_posix()\n",
    "\n",
    "sql = f\"\"\"\n",
    "WITH\n",
    "fato AS (\n",
    "  SELECT\n",
    "    Cod_Produto,\n",
    "    Chv_Cliente,\n",
    "    Chv_Vendedor,\n",
    "    DATE_TRUNC(\n",
    "      'month',\n",
    "      CAST(\n",
    "        COALESCE(\n",
    "          TRY_STRPTIME(TRIM(Dat_Entrega_Venda), '%Y-%m-%d'),\n",
    "          TRY_STRPTIME(TRIM(Dat_Entrega_Venda), '%d/%m/%Y')\n",
    "        ) AS DATE\n",
    "      )\n",
    "    ) AS PERIODO,\n",
    "    TRIM(Nom_Empresa) AS EMPRESA,\n",
    "    SUM(TRY_CAST(Qtd_Venda AS DOUBLE)) AS QTD_VENDA,\n",
    "    SUM(TRY_CAST(Qtd_Peso_Venda AS DOUBLE)) AS VOL_VENDA\n",
    "  FROM parquet_scan('{fact}')\n",
    "  WHERE UPPER(TRIM(Nom_Empresa)) LIKE '%{empresa.strip().upper()}%'\n",
    "    AND UPPER(TRIM(Des_Origem))  LIKE '%{empresa.strip().upper()}%'\n",
    "    AND Cod_Empresa IN ('01','05','08','0802','10')\n",
    "    AND TRY_CAST(NULLIF(TRIM(Cod_Bloqueio), '') AS INTEGER) IN (80,90,95,99)\n",
    "    AND TRY_CAST(Qtd_Venda AS DOUBLE) > 0\n",
    "    AND Dat_Entrega_Venda IS NOT NULL\n",
    "    AND TRIM(Dat_Entrega_Venda) <> ''\n",
    "    AND COALESCE(\n",
    "      TRY_STRPTIME(TRIM(Dat_Entrega_Venda), '%Y-%m-%d'),\n",
    "      TRY_STRPTIME(TRIM(Dat_Entrega_Venda), '%d/%m/%Y')\n",
    "    ) >= DATE '2022-01-01'\n",
    "    -- EXCLUI MÃŠS ATUAL\n",
    "    AND DATE_TRUNC(\n",
    "      'month',\n",
    "      CAST(\n",
    "        COALESCE(\n",
    "          TRY_STRPTIME(TRIM(Dat_Entrega_Venda), '%Y-%m-%d'),\n",
    "          TRY_STRPTIME(TRIM(Dat_Entrega_Venda), '%d/%m/%Y')\n",
    "        ) AS DATE\n",
    "      )\n",
    "    ) < DATE_TRUNC('month', CURRENT_DATE)\n",
    "  GROUP BY Cod_Produto, Chv_Cliente, Chv_Vendedor, PERIODO, EMPRESA\n",
    "),\n",
    "prod AS (\n",
    "  SELECT\n",
    "    Cod_Produto,\n",
    "    TRIM(Des_Produto) AS Des_Produto,\n",
    "    Cod_Familia,\n",
    "    TRIM(Des_Familia) AS Des_Familia,\n",
    "    Cod_Linha,\n",
    "    TRIM(Des_Linha) AS Des_Linha,\n",
    "    TRIM(Nom_Empresa) AS EMPRESA,\n",
    "    TRY_CAST(Num_Peso AS DOUBLE) AS PESO_UNIT\n",
    "  FROM parquet_scan('{prod}')\n",
    "  WHERE Des_Linha IS NOT NULL\n",
    "    AND TRIM(Des_Linha) <> ''\n",
    "    AND Cod_Empresa IN ('01','05','08','0802','10')\n",
    "),\n",
    "cli AS (\n",
    "  SELECT\n",
    "    Chv_Cliente,\n",
    "    TRIM(Nom_Cliente) AS NOME_CLIENTE,\n",
    "    TRIM(Nom_Empresa) AS EMPRESA,\n",
    "    Chv_Vendedor_Cliente,\n",
    "    TRIM(Des_Segmento) AS SEGMENTO,\n",
    "    -- Corrige COD_GRUPO_CLIENTE: se vazio, usa COD_CLIENTE\n",
    "    CASE\n",
    "      WHEN TRIM(Cod_Grupo_Cliente) = '' OR Cod_Grupo_Cliente IS NULL\n",
    "      THEN TRIM(SPLIT_PART(Chv_Cliente, '|', 2))\n",
    "      ELSE TRIM(Cod_Grupo_Cliente)\n",
    "    END AS COD_GRUPO_CLIENTE,\n",
    "    -- Corrige DESC_GRUPO_E_CLIENTE: se vazio, usa NOME_CLIENTE\n",
    "    CASE\n",
    "      WHEN TRIM(Des_Grupo_e_Cliente) = '' OR Des_Grupo_e_Cliente IS NULL\n",
    "      THEN TRIM(Nom_Cliente)\n",
    "      ELSE TRIM(Des_Grupo_e_Cliente)\n",
    "    END AS DESC_GRUPO_E_CLIENTE\n",
    "  FROM parquet_scan('{cli}')\n",
    "),\n",
    "vend AS (\n",
    "  SELECT\n",
    "    Chv_Vendedor,\n",
    "    TRIM(Des_Regiao) AS Des_Regiao\n",
    "  FROM parquet_scan('{vend}')\n",
    "),\n",
    "final AS (\n",
    "  SELECT\n",
    "    f.EMPRESA,\n",
    "    TRIM(SPLIT_PART(c.Chv_Cliente, '|', 2)) AS COD_CLIENTE,\n",
    "    c.NOME_CLIENTE,\n",
    "    c.COD_GRUPO_CLIENTE,\n",
    "    c.DESC_GRUPO_E_CLIENTE,\n",
    "    c.SEGMENTO,\n",
    "    f.Cod_Produto AS COD_PROD,\n",
    "    p.Des_Produto AS DESC_PRODUTO,\n",
    "    CAST(p.Cod_Familia AS VARCHAR) || ' - ' || p.Des_Familia AS FAMILIA,\n",
    "    CAST(p.Cod_Linha   AS VARCHAR) || ' - ' || p.Des_Linha   AS LINHA,\n",
    "    v1.Des_Regiao AS REGIAO_CLIENTE,\n",
    "    v2.Des_Regiao AS REGIAO_MOVIMENTO,\n",
    "    f.PERIODO,\n",
    "    f.QTD_VENDA,\n",
    "    f.VOL_VENDA,\n",
    "    f.VOL_VENDA / f.QTD_VENDA AS PESO_UNIT\n",
    "  FROM fato f\n",
    "  LEFT JOIN prod p ON f.Cod_Produto = p.Cod_Produto AND f.EMPRESA = p.EMPRESA\n",
    "  LEFT JOIN cli  c ON f.Chv_Cliente = c.Chv_Cliente AND f.EMPRESA = c.EMPRESA\n",
    "  LEFT JOIN vend v1 ON c.Chv_Vendedor_Cliente = v1.Chv_Vendedor\n",
    "  LEFT JOIN vend v2 ON f.Chv_Vendedor         = v2.Chv_Vendedor\n",
    ")\n",
    "SELECT\n",
    "  UPPER(EMPRESA) AS EMPRESA,\n",
    "  COD_CLIENTE,\n",
    "  NOME_CLIENTE,\n",
    "  COD_GRUPO_CLIENTE,\n",
    "  DESC_GRUPO_E_CLIENTE,\n",
    "  SEGMENTO,\n",
    "  COD_PROD,\n",
    "  DESC_PRODUTO,\n",
    "  FAMILIA,\n",
    "  LINHA,\n",
    "  PESO_UNIT,\n",
    "  REGIAO_CLIENTE,\n",
    "  REGIAO_MOVIMENTO,\n",
    "  PERIODO,\n",
    "  QTD_VENDA,\n",
    "  VOL_VENDA\n",
    "FROM final\n",
    "\"\"\"\n",
    "df_vendas_krona_silver = duckdb.query(sql).to_df()\n",
    "\n",
    "# Salvar em parquet\n",
    "df_vendas_krona_silver.to_parquet(pasta_staging_parquet / \"df_vendas_krona_silver.parquet\", index=False)\n",
    "\n",
    "# del df_vendas_krona_silver\n",
    "# gc.collect()\n",
    "\n",
    "print(\"âœ… Carregamento de df_vendas_krona_silver concluÃ­do com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41f4d87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f25a59dd6a1489581d3e545ca4bdbf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OrganizaÃ§Ã£o de Regionais e InserÃ§Ã£o de Regional Gestor na df_vendas_krona_gold concluÃ­dos com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1. Criando coluna REGIONAL copiando a coluna REGIAO_CLIENTE \n",
    "#    no df_vendas_krona. \n",
    "#    Onde o segmento contÃ©m CONSTRUTORA ou INSTALADOR, buscar \n",
    "#    na tabela de regionais_construtora a regional atualizada.\n",
    "# ============================================================\n",
    "\n",
    "# Carregar o df_vendas_krona_silver do parquet\n",
    "df_vendas_krona_gold = pd.read_parquet(pasta_staging_parquet / \"df_vendas_krona_silver.parquet\")\n",
    "\n",
    "# Cria a tabela de de-para das regionais (jÃ¡ registrada no engine)\n",
    "duckdb.register(\"vendas\", df_vendas_krona_gold)\n",
    "duckdb.register(\"map_reg\", df_regionais_construtora[['REGIONAL BASE','REGIONAL ATUALIZADA']])\n",
    "duckdb.register(\"direc_cli_regional\", df_direc_cli_regional[['COD_CLIENTE', 'REGIONAL']])\n",
    "\n",
    "sql = \"\"\"\n",
    "WITH base AS (\n",
    "  SELECT\n",
    "    v.*,\n",
    "    -- Substitui valores vazios de REGIAO_CLIENTE por REGIAO_MOVIMENTO\n",
    "    COALESCE(NULLIF(v.REGIAO_CLIENTE,''), v.REGIAO_MOVIMENTO) AS RC_FIX,\n",
    "    UPPER(v.SEGMENTO) AS SEG_UP,\n",
    "    UPPER(v.REGIAO_CLIENTE) AS RC,\n",
    "    UPPER(v.REGIAO_MOVIMENTO) AS RM\n",
    "  FROM vendas v\n",
    "),\n",
    "\n",
    "ajuste AS (\n",
    "  SELECT\n",
    "    b.*,\n",
    "    CASE\n",
    "      -- >>> ADICIONADO: override por cliente (se existir na df_direc_cli_regional)\n",
    "      WHEN d.REGIONAL IS NOT NULL AND d.REGIONAL <> '' THEN d.REGIONAL\n",
    "      \n",
    "      -- 1) Se SEGMENTO contÃ©m CONSTRUTORA ou INSTALADOR => usa de-para\n",
    "      WHEN b.SEG_UP LIKE '%CONSTRUTORA%' OR b.SEG_UP LIKE '%INSTALADOR%'\n",
    "        THEN COALESCE(m.\"REGIONAL ATUALIZADA\", b.RC_FIX)\n",
    "      -- ============================================================\n",
    "      -- 2. Converter TELEVENDAS - Regras para definir REGIONAL:\n",
    "      --    REGIONAL = CONSTRUTORA => REGIONAL_CONSTRUTORA\n",
    "      --    REGIAO_CLIENTE = TELEVENDAS e REGIAO_MOVIMENTO = TELEVENDAS => TELEVENDAS\n",
    "      --    REGIAO_CLIENTE != TELEVENDAS e REGIAO_MOVIMENTO = TELEVENDAS => TELEVENDAS\n",
    "      --    REGIAO_CLIENTE = TELEVENDAS e REGIAO_MOVIMENTO != TELEVENDAS => REGIAO_MOVIMENTO\n",
    "      --    Caso contrÃ¡rio => REGIAO_CLIENTE\n",
    "      -- ============================================================\n",
    "      WHEN b.RC='TELEVENDAS' AND b.RM='TELEVENDAS' THEN 'TELEVENDAS'\n",
    "      WHEN b.RC<>'TELEVENDAS' AND b.RM='TELEVENDAS' THEN 'TELEVENDAS'\n",
    "      WHEN b.RC='TELEVENDAS' AND b.RM<>'TELEVENDAS' THEN b.RM\n",
    "      ELSE b.RC_FIX\n",
    "    END AS REGIONAL\n",
    "  FROM base b\n",
    "  LEFT JOIN map_reg m\n",
    "    ON m.\"REGIONAL BASE\" = b.REGIAO_CLIENTE\n",
    "  -- >>> ADICIONADO: join com regional direcionada por cliente\n",
    "  LEFT JOIN direc_cli_regional d\n",
    "    ON d.COD_CLIENTE = b.COD_CLIENTE\n",
    ")\n",
    "\n",
    "-- ============================================================\n",
    "-- Resultado final consolidado\n",
    "-- ============================================================\n",
    "SELECT\n",
    "  EMPRESA,\n",
    "  COD_CLIENTE,\n",
    "  NOME_CLIENTE,\n",
    "  COD_GRUPO_CLIENTE,\n",
    "  DESC_GRUPO_E_CLIENTE,\n",
    "  COD_PROD,\n",
    "  DESC_PRODUTO,\n",
    "  FAMILIA,\n",
    "  LINHA,\n",
    "  REGIONAL,\n",
    "  PERIODO,\n",
    "  SUM(QTD_VENDA) AS QTD_VENDA,\n",
    "  SUM(VOL_VENDA) AS VOL_VENDA\n",
    "FROM ajuste\n",
    "-- WHERE REGIONAL IS NOT NULL AND REGIONAL <> ''\n",
    "GROUP BY\n",
    "  EMPRESA,\n",
    "  COD_CLIENTE,\n",
    "  NOME_CLIENTE,\n",
    "  COD_GRUPO_CLIENTE,\n",
    "  DESC_GRUPO_E_CLIENTE,\n",
    "  COD_PROD,\n",
    "  DESC_PRODUTO,\n",
    "  FAMILIA,\n",
    "  LINHA,\n",
    "  REGIONAL,\n",
    "  PERIODO\n",
    "\"\"\"\n",
    "\n",
    "# Executa no DuckDB\n",
    "df_vendas_krona_gold= duckdb.query(sql).to_df()\n",
    "\n",
    "# Inserir REGIONAL_GESTOR no df_vendas_krona\n",
    "df_vendas_krona_gold = pd.merge(\n",
    "    df_vendas_krona_gold,\n",
    "    df_regionais_gestor,\n",
    "    left_on='REGIONAL',\n",
    "    right_on='REGIONAL',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "colunas_ordenadas = [\n",
    "    \"EMPRESA\",\n",
    "    \"COD_CLIENTE\",\n",
    "    \"NOME_CLIENTE\",\n",
    "    \"COD_GRUPO_CLIENTE\",\n",
    "    \"DESC_GRUPO_E_CLIENTE\",\n",
    "    \"COD_PROD\",\n",
    "    \"DESC_PRODUTO\",\n",
    "    \"FAMILIA\",\n",
    "    \"LINHA\",\n",
    "    \"REGIONAL\",\n",
    "    \"REGIONAL_GESTOR\",\n",
    "    \"PERIODO\",\n",
    "    \"QTD_VENDA\",\n",
    "    \"VOL_VENDA\"\n",
    "]\n",
    "\n",
    "df_vendas_krona_gold = df_vendas_krona_gold[colunas_ordenadas]\n",
    "\n",
    "# Salvar df_vendas_krona_gold em Parquet para salvar as alteraÃ§Ãµes, filtros e regras aplicadas no histÃ³rico, otimizando memÃ³ria e garantindo rastreabilidade\n",
    "df_vendas_krona_gold.to_parquet(pasta_staging_parquet / \"df_vendas_krona_gold.parquet\", index=False)\n",
    "\n",
    "del df_vendas_krona_gold\n",
    "gc.collect()\n",
    "\n",
    "print(\"âœ… OrganizaÃ§Ã£o de Regionais e InserÃ§Ã£o de Regional Gestor na df_vendas_krona_gold concluÃ­dos com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e4f9464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… EliminaÃ§Ã£o de produtos concluÃ­da!\n"
     ]
    }
   ],
   "source": [
    "# Aplicar produtos a eliminar no df_vendas_krona_gold, e excluir os produtos listados na variavel produtos_a_eliminar vinda do arquivo de regras de negÃ³cio\n",
    "df_vendas_krona = pd.read_parquet(pasta_staging_parquet / \"df_vendas_krona_gold.parquet\")\n",
    "lista_produtos_eliminar = set(produtos_a_eliminar['COD_PROD'])\n",
    "df_vendas_krona = df_vendas_krona[~df_vendas_krona['COD_PROD'].isin(lista_produtos_eliminar)]\n",
    "df_vendas_krona.to_parquet(pasta_staging_parquet / \"df_vendas_krona.parquet\", index=False)\n",
    "\n",
    "print(\"âœ… EliminaÃ§Ã£o de produtos concluÃ­da!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0359a1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GERAR DIM_PESO_UNIT_VENDAS COM BASE NO df_vendas_krona\n",
    "\n",
    "# 1. Criar coluna PESO_UNIT divindo VOL_VENDA por QTD_VENDA\n",
    "df_dim_peso_unit_vendas = df_vendas_krona.copy()\n",
    "df_dim_peso_unit_vendas[\"PESO_UNIT\"] = df_dim_peso_unit_vendas[\"VOL_VENDA\"] / df_dim_peso_unit_vendas[\"QTD_VENDA\"]\n",
    "\n",
    "# 2. Classificar PERIODO em ordem descrescente, COD_PROD e EMPRESA crescente\n",
    "df_dim_peso_unit_vendas = df_dim_peso_unit_vendas.sort_values(by=[\"PERIODO\", \"COD_PROD\", \"EMPRESA\"], ascending=[False, True, True])\n",
    "\n",
    "# 3. Manter apenas as colunas COD_PROD, EMPRESA e PESO_UNIT\n",
    "df_dim_peso_unit_vendas = df_dim_peso_unit_vendas[[\"COD_PROD\", \"EMPRESA\", \"PESO_UNIT\"]]\n",
    "\n",
    "# 4. Remover duplicatas\n",
    "df_dim_peso_unit_vendas = df_dim_peso_unit_vendas.drop_duplicates(subset=[\"COD_PROD\", \"EMPRESA\"], keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "# 5. Salvar df_dim_peso_unit_vendas em Parquet\n",
    "df_dim_peso_unit_vendas.to_parquet(pasta_staging_parquet / \"df_dim_peso_unit_vendas.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd99a756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ¦† ExportaÃ§Ã£o de Dados Vendas para Planejamento Colaborativo\n",
    "# ðŸŽ¯ Objetivo: Exportar CSV para o Plano Colaborativo\n",
    "df_vendas_krona['NIVEL_PLAN_DEMANDA'] = np.where(\n",
    "    df_vendas_krona['COD_GRUPO_CLIENTE'].isin(lista_clientes_plan_demanda),\n",
    "    'CLIENTE',\n",
    "    'PRODUTO'\n",
    ")\n",
    "\n",
    "# Separa os DataFrames\n",
    "df_hist_vend_PRODUTO = df_vendas_krona[df_vendas_krona['NIVEL_PLAN_DEMANDA'] == 'PRODUTO']\n",
    "df_hist_vend_CLIENTE = df_vendas_krona[df_vendas_krona['NIVEL_PLAN_DEMANDA'] == 'CLIENTE']\n",
    "\n",
    "# Eliminar coluna NIVEL_PLAN_DEMANDA\n",
    "df_hist_vend_PRODUTO = df_hist_vend_PRODUTO.drop(columns=['NIVEL_PLAN_DEMANDA'])\n",
    "df_hist_vend_CLIENTE = df_hist_vend_CLIENTE.drop(columns=['NIVEL_PLAN_DEMANDA'])\n",
    "\n",
    "# Agrupar df_hist_vend_PRODUTO por REGIONAL_GESTOR, FAMILIA, PERIODO, VOL_VENDA\n",
    "df_hist_vend_PRODUTO = df_hist_vend_PRODUTO.groupby(\n",
    "    ['REGIONAL_GESTOR', 'REGIONAL', 'FAMILIA', 'PERIODO'],\n",
    "    as_index=False\n",
    ").agg({'VOL_VENDA': 'sum'}).reset_index(drop=True)\n",
    "\n",
    "# Salva como CSV\n",
    "df_hist_vend_PRODUTO.to_csv(\n",
    "    pasta_input_painel / 'HIST_VENDA_KRONA_AGREGADO.csv',\n",
    "    sep=';',\n",
    "    encoding='utf-8-sig',\n",
    "    index=False,\n",
    "    decimal=',',\n",
    "    float_format=\"%.2f\"\n",
    ")\n",
    "\n",
    "# Agrupar df_hist_vend_CLIENTE por COD_GRUPO_CLIENTE, DESC_GRUPO_E_CLIENTE, REGIONAL_GESTOR, FAMILIA, PERIODO, VOL_VENDA\n",
    "df_hist_vend_CLIENTE = df_hist_vend_CLIENTE.groupby(\n",
    "    [\"COD_GRUPO_CLIENTE\",\"DESC_GRUPO_E_CLIENTE\", \"REGIONAL_GESTOR\", 'REGIONAL', \"FAMILIA\", \"PERIODO\"],\n",
    "    as_index=False\n",
    ").agg({'VOL_VENDA': 'sum'}).reset_index(drop=True)\n",
    "\n",
    "# Salva como CSV\n",
    "df_hist_vend_CLIENTE.to_csv(\n",
    "    pasta_input_painel / 'HIST_VENDA_KRONA_CLIENTE.csv',\n",
    "    sep=';',\n",
    "    encoding='utf-8-sig',\n",
    "    index=False,\n",
    "    decimal=',',\n",
    "    float_format=\"%.2f\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edfb9c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar os arquivos com mÃ©dia de vendas para Planejamento Colaborativo Agregado\n",
    "# Encontrar o primeiro dia do mÃªs atual\n",
    "\n",
    "colunas_agregadas = ['REGIONAL_GESTOR', 'REGIONAL', 'FAMILIA']\n",
    "\n",
    "hoje = datetime.today()\n",
    "primeiro_dia_mes_atual = datetime(hoje.year, hoje.month, 1)\n",
    "\n",
    "# Calcular o primeiro dia do mÃªs de 6 meses atrÃ¡s (excluindo mÃªs atual)\n",
    "primeiro_dia_6_meses_atras = (primeiro_dia_mes_atual - pd.DateOffset(months=6)).to_pydatetime()\n",
    "\n",
    "# Filtrar apenas os Ãºltimos 6 meses (excluindo mÃªs atual)\n",
    "mask = (df_hist_vend_PRODUTO['PERIODO'] >= primeiro_dia_6_meses_atras) & (df_hist_vend_PRODUTO['PERIODO'] < primeiro_dia_mes_atual)\n",
    "df_hist_vend_PRODUTO_ultimos_6_meses = df_hist_vend_PRODUTO.loc[mask].copy()\n",
    "\n",
    "# Ordenar por data crescente\n",
    "df_hist_vend_PRODUTO_ultimos_6_meses = df_hist_vend_PRODUTO_ultimos_6_meses.sort_values('PERIODO').reset_index(drop=True)\n",
    "\n",
    "# DataFrame dos 3 meses mais recentes (Ãºltimos 3 meses do intervalo filtrado)\n",
    "df_3_meses_mais_recentes = df_hist_vend_PRODUTO_ultimos_6_meses.copy()\n",
    "\n",
    "# Identificar as 3 datas mais recentes (sem duplicar por linha)\n",
    "meses_recentes = sorted(df_3_meses_mais_recentes['PERIODO'].unique())[-3:]\n",
    "\n",
    "# Filtrar todas as linhas que pertencem a esses 3 meses\n",
    "df_3_meses_mais_recentes = df_3_meses_mais_recentes[df_3_meses_mais_recentes['PERIODO'].isin(meses_recentes)].copy()\n",
    "\n",
    "# Agrupa pelas colunas desejadas e calcula a mÃ©dia das colunas numÃ©ricas\n",
    "df_3_meses_mais_recentes_media = df_3_meses_mais_recentes.groupby(colunas_agregadas).mean(numeric_only=True).reset_index()\n",
    "\n",
    "# Adicionar coluna MEDIA informando 'MÃ‰DIA 3 MESES' na coluna\n",
    "df_3_meses_mais_recentes_media['MEDIA'] = 'MÃ‰DIA 3 MESES'\n",
    "\n",
    "# Agrupamento fazendo mÃ©dia dos 6 meses\n",
    "df_6_meses_mais_recentes_media = df_hist_vend_PRODUTO_ultimos_6_meses.copy()\n",
    "df_6_meses_mais_recentes_media = df_6_meses_mais_recentes_media.groupby(colunas_agregadas).mean(numeric_only=True).reset_index()\n",
    "\n",
    "# Adicionar coluna MEDIA informando 'MÃ‰DIA 6 MESES' na coluna\n",
    "df_6_meses_mais_recentes_media['MEDIA'] = 'MÃ‰DIA 6 MESES'\n",
    "\n",
    "# Concatenar os DataFrames\n",
    "df_media_vendas_PRODUTO = pd.concat([df_3_meses_mais_recentes_media, df_6_meses_mais_recentes_media], ignore_index=True)\n",
    "\n",
    "# Pivotar a coluna MEDIA\n",
    "df_media_vendas_PRODUTO = df_media_vendas_PRODUTO.pivot_table(\n",
    "    index=colunas_agregadas,\n",
    "    columns='MEDIA',\n",
    "    values='VOL_VENDA',\n",
    "    aggfunc='sum',\n",
    "    fill_value=0\n",
    ").reset_index()\n",
    "\n",
    "# Gerar o arquivo CSV\n",
    "df_media_vendas_PRODUTO.to_csv(\n",
    "    pasta_input_painel / 'MEDIA_VENDA_KRONA_AGREGADO.csv',\n",
    "    sep=';',\n",
    "    encoding='utf-8-sig',\n",
    "    index=False,\n",
    "    decimal=',',\n",
    "    float_format=\"%.2f\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dfd335e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Bases de Vendas para Planejamento Colaborativo geradas com sucesso!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Gerar os arquivos com mÃ©dia de vendas para Planejamento Colaborativo por Cliente\n",
    "# Encontrar o primeiro dia do mÃªs atual\n",
    "colunas_agrupadas = ['COD_GRUPO_CLIENTE', 'DESC_GRUPO_E_CLIENTE', 'REGIONAL_GESTOR', 'REGIONAL', 'FAMILIA']\n",
    "\n",
    "hoje = datetime.today()\n",
    "primeiro_dia_mes_atual = datetime(hoje.year, hoje.month, 1)\n",
    "\n",
    "# Calcular o primeiro dia do mÃªs de 6 meses atrÃ¡s (excluindo mÃªs atual)\n",
    "primeiro_dia_6_meses_atras = (primeiro_dia_mes_atual - pd.DateOffset(months=6)).to_pydatetime()\n",
    "\n",
    "# Filtrar apenas os Ãºltimos 6 meses (excluindo mÃªs atual)\n",
    "mask = (df_hist_vend_CLIENTE['PERIODO'] >= primeiro_dia_6_meses_atras) & (df_hist_vend_CLIENTE['PERIODO'] < primeiro_dia_mes_atual)\n",
    "df_hist_vend_CLIENTE_ultimos_6_meses = df_hist_vend_CLIENTE.loc[mask].copy()\n",
    "\n",
    "# Ordenar por data crescente\n",
    "df_hist_vend_CLIENTE_ultimos_6_meses = df_hist_vend_CLIENTE_ultimos_6_meses.sort_values('PERIODO').reset_index(drop=True)\n",
    "\n",
    "# DataFrame dos 3 meses mais recentes (Ãºltimos 3 meses do intervalo filtrado)\n",
    "df_3_meses_mais_recentes = df_hist_vend_CLIENTE_ultimos_6_meses.copy()\n",
    "\n",
    "# Identificar as 3 datas mais recentes (sem duplicar por linha)\n",
    "meses_recentes = sorted(df_3_meses_mais_recentes['PERIODO'].unique())[-3:]\n",
    "\n",
    "# Filtrar todas as linhas que pertencem a esses 3 meses\n",
    "df_3_meses_mais_recentes = df_3_meses_mais_recentes[df_3_meses_mais_recentes['PERIODO'].isin(meses_recentes)].copy()\n",
    "\n",
    "# Agrupa pelas colunas desejadas e calcula a mÃ©dia das colunas numÃ©ricas\n",
    "df_3_meses_mais_recentes_media = df_3_meses_mais_recentes.groupby(colunas_agrupadas).mean(numeric_only=True).reset_index()\n",
    "\n",
    "# Adicionar coluna MEDIA informando 'MÃ‰DIA 3 MESES' na coluna\n",
    "df_3_meses_mais_recentes_media['MEDIA'] = 'MÃ‰DIA 3 MESES'\n",
    "\n",
    "# Agrupamento fazendo mÃ©dia dos 6 meses\n",
    "df_6_meses_mais_recentes_media = df_hist_vend_CLIENTE_ultimos_6_meses.copy()\n",
    "df_6_meses_mais_recentes_media = df_6_meses_mais_recentes_media.groupby(colunas_agrupadas).mean(numeric_only=True).reset_index()\n",
    "\n",
    "# Adicionar coluna MEDIA informando 'MÃ‰DIA 6 MESES' na coluna\n",
    "df_6_meses_mais_recentes_media['MEDIA'] = 'MÃ‰DIA 6 MESES'\n",
    "\n",
    "# Concatenar os DataFrames\n",
    "df_media_vendas_CLIENTE = pd.concat([df_3_meses_mais_recentes_media, df_6_meses_mais_recentes_media], ignore_index=True)\n",
    "\n",
    "# Pivotar a coluna MEDIA\n",
    "df_media_vendas_CLIENTE = df_media_vendas_CLIENTE.pivot_table(\n",
    "    index=colunas_agrupadas,\n",
    "    columns='MEDIA',\n",
    "    values='VOL_VENDA',\n",
    "    aggfunc='sum',\n",
    "    fill_value=0\n",
    ").reset_index()\n",
    "\n",
    "# Validar se as colunas 'MÃ‰DIA 3 MESES' e 'MÃ‰DIA 6 MESES' existem, caso contrÃ¡rio, criar com valor 0\n",
    "if 'MÃ‰DIA 3 MESES' not in df_media_vendas_CLIENTE.columns:\n",
    "    df_media_vendas_CLIENTE['MÃ‰DIA 3 MESES'] = 0.0\n",
    "if 'MÃ‰DIA 6 MESES' not in df_media_vendas_CLIENTE.columns:\n",
    "    df_media_vendas_CLIENTE['MÃ‰DIA 6 MESES'] = 0.0\n",
    "\n",
    "# Gerar o arquivo CSV\n",
    "df_media_vendas_CLIENTE.to_csv(\n",
    "    pasta_input_painel / 'MEDIA_VENDA_KRONA_CLIENTE.csv',\n",
    "    sep=';',\n",
    "    encoding='utf-8-sig',\n",
    "    index=False,\n",
    "    decimal=',',\n",
    "    float_format=\"%.2f\"\n",
    ")\n",
    "\n",
    "del df_hist_vend_PRODUTO, df_hist_vend_CLIENTE, df_vendas_krona, produtos_a_eliminar\n",
    "gc.collect()\n",
    "\n",
    "print(\"âœ… Bases de Vendas para Planejamento Colaborativo geradas com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acf44530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nðŸ“Š Modelos de PrevisÃ£o Aplicados (VersÃ£o Final)\\n\\nEste script realiza previsÃ£o mensal por material (COD_PROD) utilizando mÃºltiplos modelos estatÃ­sticos e de\\nMachine Learning. Para cada COD_PROD, os modelos sÃ£o comparados via backtest no final do histÃ³rico e o modelo\\ncom menor erro Ã© selecionado automaticamente para gerar a projeÃ§Ã£o final.\\n\\nSeleÃ§Ã£o do melhor modelo (por COD_PROD):\\n- O histÃ³rico Ã© dividido em treino e validaÃ§Ã£o (janela final, ex.: 12 meses).\\n- Cada modelo gera previsÃµes para a janela de validaÃ§Ã£o.\\n- O erro Ã© calculado (WAPE ou MAPE, conforme configuraÃ§Ã£o do script).\\n- O modelo com menor erro Ã© escolhido para prever o horizonte futuro definido por:\\n  df_periodo_previsao[\"PERIODO_PROJECAO\"] (lista de meses jÃ¡ tratados no upstream).\\n\\nModelos utilizados:\\n\\n1) RegressÃ£o Linear (Linear Regression)\\n   - Captura: tendÃªncia linear ao longo do tempo (crescimento/queda constantes).\\n   - Uso: sÃ©ries com comportamento estÃ¡vel ou como fallback quando hÃ¡ pouco histÃ³rico.\\n   - ForÃ§a: simples, rÃ¡pida, fÃ¡cil de explicar e Ã³tima como baseline.\\n\\n2) Holt-Winters / Exponential Smoothing (com fallback)\\n   - Captura: tendÃªncia e, quando aplicÃ¡vel, sazonalidade (ex.: anual em dados mensais).\\n   - Uso: quando a sÃ©rie apresenta estrutura temporal consistente (tendÃªncia e/ou sazonalidade).\\n   - ForÃ§a: modelo clÃ¡ssico e confiÃ¡vel para sÃ©ries temporais; prioriza observaÃ§Ãµes recentes.\\n   - ObservaÃ§Ã£o: o script tenta sazonalidade multiplicativa e/ou aditiva; se falhar, cai para Holt (trend-only).\\n\\n3) ARIMA (AutoRegressive Integrated Moving Average)\\n   - Captura: autocorrelaÃ§Ã£o, dinÃ¢mica temporal e ruÃ­do estatÃ­stico da sÃ©rie.\\n   - Uso: quando hÃ¡ padrÃ£o autoregressivo e boa estabilidade temporal.\\n   - ForÃ§a: modelo estatÃ­stico tradicional, robusto e amplamente aceito em sÃ©ries temporais.\\n\\n4) Random Forest Regressor (Machine Learning)\\n   - Captura: padrÃµes nÃ£o lineares e interaÃ§Ãµes entre variÃ¡veis temporais (ex.: mÃªs/ano e tempo).\\n   - Uso: sÃ©ries com comportamento irregular, nÃ£o linear ou com variaÃ§Ãµes que modelos lineares nÃ£o capturam bem.\\n   - ForÃ§a: robusto a ruÃ­dos, bom desempenho em cenÃ¡rios complexos e agrega â€œMLâ€ ao portfÃ³lio tÃ©cnico.\\n\\n5) Gradient Boosting Regressor (Machine Learning)\\n   - Captura: relaÃ§Ãµes complexas e refinamentos de padrÃ£o, frequentemente com boa precisÃ£o preditiva.\\n   - Uso: quando hÃ¡ nÃ£o-linearidade, mudanÃ§as de regime e necessidade de um modelo ML mais â€œforteâ€.\\n   - ForÃ§a: alto poder preditivo, muito usado em problemas de previsÃ£o com features temporais.\\n\\n6) LinearRegression_Fallback (EstratÃ©gia de contingÃªncia)\\n   - Captura: tendÃªncia simples quando nÃ£o hÃ¡ histÃ³rico suficiente para modelos mais robustos.\\n   - Uso: ativado automaticamente em COD_PROD com histÃ³rico curto ou quando os demais modelos falham.\\n   - ForÃ§a: garante continuidade do processo sem interromper a geraÃ§Ã£o da projeÃ§Ã£o.\\n\\nSaÃ­da:\\n- A previsÃ£o final Ã© incorporada ao dataset junto ao histÃ³rico.\\n- A coluna \"MODELO_ESCOLHIDO\" registra qual modelo foi selecionado para cada COD_PROD no horizonte previsto.\\n- A coluna de valor (ex.: \"VOL_VENDA_REAL\") contÃ©m histÃ³rico + projeÃ§Ã£o no mesmo padrÃ£o mensal.\\n\\nOBSERVAÃ‡ÃƒO:\\n- O mesmo modelo que foi escolhido para prever o futuro foi testado nos Ãºltimos meses do histÃ³rico.\\nIsso nos permite medir a assertividade do modelo antes de usÃ¡-lo para projeÃ§Ã£o.\\n- Aplicado APE para identificar o erro de cada previsÃ£o definida no backtest. APE=(Real âˆ’ Previsto)/Real\\n- AlÃ©m da projeÃ§Ã£o, avaliamos a precisÃ£o histÃ³rica de cada modelo por produto, garantindo confiabilidade estatÃ­stica nas previsÃµes.\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "ðŸ§© HistÃ³rico dos Modelos Testados no Projeto\n",
    "--------------------------------------------\n",
    "\n",
    "1. MÃ©dia Simples / MÃ©dia 12M\n",
    "   DescriÃ§Ã£o: cÃ¡lculo da mÃ©dia das vendas dos Ãºltimos 12 meses.\n",
    "   Objetivo: criar um ponto de partida rÃ¡pido para testar estabilidade.\n",
    "   Vantagem: extremamente leve e previsÃ­vel.\n",
    "   LimitaÃ§Ã£o: ignora tendÃªncias (crescimento ou queda) e nÃ£o reage a sazonalidades.\n",
    "\n",
    "2. MÃ©dia MÃ³vel Ponderada\n",
    "   DescriÃ§Ã£o: mÃ©dia dos Ãºltimos 12 meses com pesos maiores para os meses mais recentes.\n",
    "   Objetivo: suavizar o histÃ³rico sem perder sensibilidade Ã  tendÃªncia recente.\n",
    "   Vantagem: melhora ligeiramente a resposta a movimentos recentes.\n",
    "   LimitaÃ§Ã£o: ainda nÃ£o reconhece padrÃµes anuais completos de sazonalidade.\n",
    "\n",
    "3. Sazonalidade Percentual HistÃ³rica\n",
    "   DescriÃ§Ã£o: calculava a participaÃ§Ã£o mÃ©dia de cada mÃªs no total anual.\n",
    "   Objetivo: reproduzir o comportamento sazonal real da empresa.\n",
    "   Vantagem: respeita picos e vales mensais do Ãºltimo ano completo.\n",
    "   LimitaÃ§Ã£o: dependente da qualidade do Ãºltimo ano â€” nÃ£o projeta volume total, apenas distribui.\n",
    "\n",
    "4. LightGBM\n",
    "   DescriÃ§Ã£o: modelo de machine learning (boosting de Ã¡rvores) aplicado sobre variÃ¡veis sazonais (seno/cosseno dos meses).\n",
    "   Objetivo: prever volumes mensais aprendendo padrÃµes nÃ£o lineares.\n",
    "   Vantagem: aprendizado rÃ¡pido e robusto em bases amplas.\n",
    "   LimitaÃ§Ã£o: exige ajuste fino e mais dados; em sÃ©ries curtas, tende a superajustar.\n",
    "\n",
    "5. RegressÃ£o Linear (nÃ­vel anual)\n",
    "   DescriÃ§Ã£o: ajusta uma reta sobre as vendas anuais (y = aÂ·x + b).\n",
    "   Objetivo: capturar tendÃªncias de crescimento ou queda sustentadas.\n",
    "   Vantagem: intuitivo e fÃ¡cil de justificar visualmente.\n",
    "   LimitaÃ§Ã£o: nÃ£o lida bem com oscilaÃ§Ãµes bruscas ou sÃ©ries curtas.\n",
    "\n",
    "6. Holt-Winters Aditivo\n",
    "   DescriÃ§Ã£o: modelo clÃ¡ssico de sÃ©ries temporais com nÃ­vel, tendÃªncia e sazonalidade (additive trend + seasonal).\n",
    "   Objetivo: gerar previsÃµes suaves mantendo padrÃ£o anual.\n",
    "   Vantagem: reconhecido e equilibrado entre suavidade e tendÃªncia.\n",
    "   LimitaÃ§Ã£o: pesado em grandes volumes e instÃ¡vel em sÃ©ries curtas.\n",
    "\n",
    "7. Ensemble EstatÃ­stico (fase intermediÃ¡ria)\n",
    "   DescriÃ§Ã£o: combinaÃ§Ã£o ponderada de modelos simples (RegressÃ£o + MÃ©dia + SuavizaÃ§Ã£o).\n",
    "   Objetivo: estabilizar volumes sem perder aparÃªncia estatÃ­stica.\n",
    "   Vantagem: resultados consistentes e realistas.\n",
    "   LimitaÃ§Ã£o: apresentava sempre o mesmo nome, sem variaÃ§Ã£o por empresa.\n",
    "      \n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "ðŸ“Š Modelos de PrevisÃ£o Aplicados (VersÃ£o Final)\n",
    "\n",
    "Este script realiza previsÃ£o mensal por material (COD_PROD) utilizando mÃºltiplos modelos estatÃ­sticos e de\n",
    "Machine Learning. Para cada COD_PROD, os modelos sÃ£o comparados via backtest no final do histÃ³rico e o modelo\n",
    "com menor erro Ã© selecionado automaticamente para gerar a projeÃ§Ã£o final.\n",
    "\n",
    "SeleÃ§Ã£o do melhor modelo (por COD_PROD):\n",
    "- O histÃ³rico Ã© dividido em treino e validaÃ§Ã£o (janela final, ex.: 12 meses).\n",
    "- Cada modelo gera previsÃµes para a janela de validaÃ§Ã£o.\n",
    "- O erro Ã© calculado (WAPE ou MAPE, conforme configuraÃ§Ã£o do script).\n",
    "- O modelo com menor erro Ã© escolhido para prever o horizonte futuro definido por:\n",
    "  df_periodo_previsao[\"PERIODO_PROJECAO\"] (lista de meses jÃ¡ tratados no upstream).\n",
    "\n",
    "Modelos utilizados:\n",
    "\n",
    "1) RegressÃ£o Linear (Linear Regression)\n",
    "   - Captura: tendÃªncia linear ao longo do tempo (crescimento/queda constantes).\n",
    "   - Uso: sÃ©ries com comportamento estÃ¡vel ou como fallback quando hÃ¡ pouco histÃ³rico.\n",
    "   - ForÃ§a: simples, rÃ¡pida, fÃ¡cil de explicar e Ã³tima como baseline.\n",
    "\n",
    "2) Holt-Winters / Exponential Smoothing (com fallback)\n",
    "   - Captura: tendÃªncia e, quando aplicÃ¡vel, sazonalidade (ex.: anual em dados mensais).\n",
    "   - Uso: quando a sÃ©rie apresenta estrutura temporal consistente (tendÃªncia e/ou sazonalidade).\n",
    "   - ForÃ§a: modelo clÃ¡ssico e confiÃ¡vel para sÃ©ries temporais; prioriza observaÃ§Ãµes recentes.\n",
    "   - ObservaÃ§Ã£o: o script tenta sazonalidade multiplicativa e/ou aditiva; se falhar, cai para Holt (trend-only).\n",
    "\n",
    "3) ARIMA (AutoRegressive Integrated Moving Average)\n",
    "   - Captura: autocorrelaÃ§Ã£o, dinÃ¢mica temporal e ruÃ­do estatÃ­stico da sÃ©rie.\n",
    "   - Uso: quando hÃ¡ padrÃ£o autoregressivo e boa estabilidade temporal.\n",
    "   - ForÃ§a: modelo estatÃ­stico tradicional, robusto e amplamente aceito em sÃ©ries temporais.\n",
    "\n",
    "4) Random Forest Regressor (Machine Learning)\n",
    "   - Captura: padrÃµes nÃ£o lineares e interaÃ§Ãµes entre variÃ¡veis temporais (ex.: mÃªs/ano e tempo).\n",
    "   - Uso: sÃ©ries com comportamento irregular, nÃ£o linear ou com variaÃ§Ãµes que modelos lineares nÃ£o capturam bem.\n",
    "   - ForÃ§a: robusto a ruÃ­dos, bom desempenho em cenÃ¡rios complexos e agrega â€œMLâ€ ao portfÃ³lio tÃ©cnico.\n",
    "\n",
    "5) Gradient Boosting Regressor (Machine Learning)\n",
    "   - Captura: relaÃ§Ãµes complexas e refinamentos de padrÃ£o, frequentemente com boa precisÃ£o preditiva.\n",
    "   - Uso: quando hÃ¡ nÃ£o-linearidade, mudanÃ§as de regime e necessidade de um modelo ML mais â€œforteâ€.\n",
    "   - ForÃ§a: alto poder preditivo, muito usado em problemas de previsÃ£o com features temporais.\n",
    "   \n",
    "6) LinearRegression_Fallback (EstratÃ©gia de contingÃªncia)\n",
    "   - Captura: tendÃªncia simples quando nÃ£o hÃ¡ histÃ³rico suficiente para modelos mais robustos.\n",
    "   - Uso: ativado automaticamente em COD_PROD com histÃ³rico curto ou quando os demais modelos falham.\n",
    "   - ForÃ§a: garante continuidade do processo sem interromper a geraÃ§Ã£o da projeÃ§Ã£o.\n",
    "\n",
    "SaÃ­da:\n",
    "- A previsÃ£o final Ã© incorporada ao dataset junto ao histÃ³rico.\n",
    "- A coluna \"MODELO_ESCOLHIDO\" registra qual modelo foi selecionado para cada COD_PROD no horizonte previsto.\n",
    "- A coluna de valor (ex.: \"VOL_VENDA_REAL\") contÃ©m histÃ³rico + projeÃ§Ã£o no mesmo padrÃ£o mensal.\n",
    "\n",
    "OBSERVAÃ‡ÃƒO:\n",
    "- O mesmo modelo que foi escolhido para prever o futuro foi testado nos Ãºltimos meses do histÃ³rico.\n",
    "Isso nos permite medir a assertividade do modelo antes de usÃ¡-lo para projeÃ§Ã£o.\n",
    "- Aplicado APE para identificar o erro de cada previsÃ£o definida no backtest. APE=(Real âˆ’ Previsto)/Real\n",
    "- AlÃ©m da projeÃ§Ã£o, avaliamos a precisÃ£o histÃ³rica de cada modelo por produto, garantindo confiabilidade estatÃ­stica nas previsÃµes.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea423ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Iniciando processo de previsÃ£o estatÃ­stica...\n",
      "ðŸ“¦ df_vendas_krona carregado | Linhas: 11,105,253\n",
      "ðŸ“Š Dados agregados | SKUs: 1,180 | PerÃ­odos: 49\n",
      "ðŸ—“ï¸ Horizonte de previsÃ£o | Meses: 4 | 2026-03-01 â†’ 2026-06-01\n",
      "ðŸš€ Iniciando previsÃ£o por SKU | Total: 1,180\n",
      "   â–¶ï¸ Processando SKU 1/1180 | COD_PROD=0023\n",
      "   â–¶ï¸ Processando SKU 50/1180 | COD_PROD=0152\n",
      "   â–¶ï¸ Processando SKU 100/1180 | COD_PROD=0240\n",
      "   â–¶ï¸ Processando SKU 150/1180 | COD_PROD=0313\n",
      "   â–¶ï¸ Processando SKU 200/1180 | COD_PROD=0378\n",
      "   â–¶ï¸ Processando SKU 250/1180 | COD_PROD=0432\n",
      "   â–¶ï¸ Processando SKU 300/1180 | COD_PROD=0484\n",
      "   â–¶ï¸ Processando SKU 350/1180 | COD_PROD=0561\n",
      "   â–¶ï¸ Processando SKU 400/1180 | COD_PROD=0621\n",
      "   â–¶ï¸ Processando SKU 450/1180 | COD_PROD=0677\n",
      "   â–¶ï¸ Processando SKU 500/1180 | COD_PROD=0738\n",
      "   â–¶ï¸ Processando SKU 550/1180 | COD_PROD=0806\n",
      "   â–¶ï¸ Processando SKU 600/1180 | COD_PROD=0928\n",
      "   â–¶ï¸ Processando SKU 650/1180 | COD_PROD=1031\n",
      "   â–¶ï¸ Processando SKU 700/1180 | COD_PROD=1143\n",
      "   â–¶ï¸ Processando SKU 750/1180 | COD_PROD=1232\n",
      "   â–¶ï¸ Processando SKU 800/1180 | COD_PROD=1305\n",
      "   â–¶ï¸ Processando SKU 850/1180 | COD_PROD=1357\n",
      "   â–¶ï¸ Processando SKU 900/1180 | COD_PROD=1420\n",
      "   â–¶ï¸ Processando SKU 950/1180 | COD_PROD=1489\n",
      "   â–¶ï¸ Processando SKU 1000/1180 | COD_PROD=1549\n",
      "   â–¶ï¸ Processando SKU 1050/1180 | COD_PROD=1599\n",
      "   â–¶ï¸ Processando SKU 1100/1180 | COD_PROD=1821\n",
      "   â–¶ï¸ Processando SKU 1150/1180 | COD_PROD=1949\n",
      "ðŸ§© Finalizando consolidaÃ§Ã£o de resultados (forecast futuro)...\n",
      "ðŸ“¦ Dataset base montado (histÃ³rico + futuro).\n",
      "ðŸ§ª Iniciando backtest completo (walk-forward) por SKU...\n",
      "ðŸ”Ž Backtest completo | Total SKUs: 1,180 | MIN_TREINO=12\n",
      "   â–¶ï¸ Backtest SKU 1/1180 | COD_PROD=0023\n",
      "   â–¶ï¸ Backtest SKU 50/1180 | COD_PROD=0152\n",
      "   â–¶ï¸ Backtest SKU 100/1180 | COD_PROD=0240\n",
      "   â–¶ï¸ Backtest SKU 150/1180 | COD_PROD=0313\n",
      "   â–¶ï¸ Backtest SKU 200/1180 | COD_PROD=0378\n",
      "   â–¶ï¸ Backtest SKU 250/1180 | COD_PROD=0432\n",
      "   â–¶ï¸ Backtest SKU 300/1180 | COD_PROD=0484\n",
      "   â–¶ï¸ Backtest SKU 350/1180 | COD_PROD=0561\n",
      "   â–¶ï¸ Backtest SKU 400/1180 | COD_PROD=0621\n",
      "   â–¶ï¸ Backtest SKU 450/1180 | COD_PROD=0677\n",
      "   â–¶ï¸ Backtest SKU 500/1180 | COD_PROD=0738\n",
      "   â–¶ï¸ Backtest SKU 550/1180 | COD_PROD=0806\n",
      "   â–¶ï¸ Backtest SKU 600/1180 | COD_PROD=0928\n",
      "   â–¶ï¸ Backtest SKU 650/1180 | COD_PROD=1031\n",
      "   â–¶ï¸ Backtest SKU 700/1180 | COD_PROD=1143\n",
      "   â–¶ï¸ Backtest SKU 750/1180 | COD_PROD=1232\n",
      "   â–¶ï¸ Backtest SKU 800/1180 | COD_PROD=1305\n",
      "   â–¶ï¸ Backtest SKU 850/1180 | COD_PROD=1357\n",
      "   â–¶ï¸ Backtest SKU 900/1180 | COD_PROD=1420\n",
      "   â–¶ï¸ Backtest SKU 950/1180 | COD_PROD=1489\n",
      "   â–¶ï¸ Backtest SKU 1000/1180 | COD_PROD=1549\n",
      "   â–¶ï¸ Backtest SKU 1050/1180 | COD_PROD=1599\n",
      "   â–¶ï¸ Backtest SKU 1100/1180 | COD_PROD=1821\n",
      "   â–¶ï¸ Backtest SKU 1150/1180 | COD_PROD=1949\n",
      "âœ… Backtest completo concluÃ­do (histÃ³rico preenchido).\n",
      "âœ… Finalizado e salvo: df_forecast_estatistico_krona.csv\n"
     ]
    }
   ],
   "source": [
    "# PREVISAO ESTATISTICA â€” 5 MODELOS + MELHOR POR COD_PROD (SEM TRATAMENTO EXTRA)\n",
    "print(\"ðŸ”„ Iniciando processo de previsÃ£o estatÃ­stica...\")\n",
    "\n",
    "# ============================================================\n",
    "# 0) CARREGAR df_vendas_krona DO PARQUET (para limpar memÃ³ria)\n",
    "# ============================================================\n",
    "\n",
    "df_vendas_krona = pd.read_parquet(pasta_staging_parquet / \"df_vendas_krona.parquet\")\n",
    "print(f\"ðŸ“¦ df_vendas_krona carregado | Linhas: {len(df_vendas_krona):,}\")\n",
    "\n",
    "# ============================================================\n",
    "# 1) AGRUPAMENTO PADRÃƒO\n",
    "# ============================================================\n",
    "\n",
    "df_group = (\n",
    "    df_vendas_krona\n",
    "    .groupby([\"COD_PROD\", \"PERIODO\"], as_index=False)\n",
    "    .agg(VOL_VENDA=(\"VOL_VENDA\", \"sum\"))\n",
    "    .sort_values([\"COD_PROD\", \"PERIODO\"])\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"ðŸ“Š Dados agregados | SKUs: {df_group['COD_PROD'].nunique():,} | \"\n",
    "    f\"PerÃ­odos: {df_group['PERIODO'].nunique():,}\"\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 2) CALENDÃRIO FUTURO\n",
    "# ============================================================\n",
    "\n",
    "future_dates = pd.DatetimeIndex(\n",
    "    df_periodo_previsao[\"PERIODO_PROJECAO\"].drop_duplicates().sort_values()\n",
    ")\n",
    "\n",
    "if len(future_dates) == 0:\n",
    "    raise ValueError(\"df_periodo_previsao['PERIODO_PROJECAO'] estÃ¡ vazio.\")\n",
    "\n",
    "primeiro_mes_previsao = future_dates.min()\n",
    "ultimo_mes_hist = primeiro_mes_previsao - pd.offsets.MonthBegin(1)\n",
    "\n",
    "df_hist_base = df_group[df_group[\"PERIODO\"] <= ultimo_mes_hist].copy()\n",
    "if df_hist_base.empty:\n",
    "    raise ValueError(\"HistÃ³rico vazio apÃ³s corte pelo calendÃ¡rio futuro.\")\n",
    "\n",
    "horizon = len(future_dates)\n",
    "\n",
    "print(\n",
    "    f\"ðŸ—“ï¸ Horizonte de previsÃ£o | Meses: {horizon} | \"\n",
    "    f\"{future_dates.min().date()} â†’ {future_dates.max().date()}\"\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 3) MÃ‰TRICA (WAPE ou MAPE)\n",
    "# ============================================================\n",
    "\n",
    "METRICA_USADA = \"WAPE\"  # \"WAPE\" recomendado; se quiser \"MAPE\", troque aqui.\n",
    "\n",
    "def wape(y_true, y_pred) -> float:\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    denom = np.sum(np.abs(y_true))\n",
    "    if denom == 0:\n",
    "        return float(np.mean(np.abs(y_true - y_pred)))\n",
    "    return float(np.sum(np.abs(y_true - y_pred)) / denom)\n",
    "\n",
    "def safe_mape(y_true, y_pred) -> float:\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    mask = y_true != 0\n",
    "    if mask.sum() == 0:\n",
    "        return float(np.mean(np.abs(y_true - y_pred)))\n",
    "    return float(mean_absolute_percentage_error(y_true[mask], y_pred[mask]))\n",
    "\n",
    "def metric(y_true, y_pred) -> float:\n",
    "    return wape(y_true, y_pred) if METRICA_USADA == \"WAPE\" else safe_mape(y_true, y_pred)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) MODELOS\n",
    "# ============================================================\n",
    "\n",
    "def pred_hw(y_train, steps):\n",
    "    # tenta sazonal; se falhar, cai pra Holt trend\n",
    "    try:\n",
    "        m = ExponentialSmoothing(y_train, trend=\"add\", seasonal=\"multiplicative\", seasonal_periods=12).fit()\n",
    "        return np.maximum(m.forecast(steps), 0)\n",
    "    except Exception:\n",
    "        try:\n",
    "            m = ExponentialSmoothing(y_train, trend=\"add\", seasonal=\"additive\", seasonal_periods=12).fit()\n",
    "            return np.maximum(m.forecast(steps), 0)\n",
    "        except Exception:\n",
    "            m = ExponentialSmoothing(y_train, trend=\"add\", seasonal=None).fit()\n",
    "            return np.maximum(m.forecast(steps), 0)\n",
    "\n",
    "def pred_arima(y_train, steps):\n",
    "    m = ARIMA(y_train, order=(1,1,1)).fit()\n",
    "    return np.maximum(m.forecast(steps), 0)\n",
    "\n",
    "def pred_lr(y_train, steps):\n",
    "    y_train = np.asarray(y_train, dtype=float)\n",
    "    t = np.arange(len(y_train)).reshape(-1, 1)\n",
    "    lr = LinearRegression().fit(t, y_train)\n",
    "    t_future = np.arange(len(y_train), len(y_train) + steps).reshape(-1, 1)\n",
    "    return np.maximum(lr.predict(t_future), 0)\n",
    "\n",
    "def make_features_simple(y):\n",
    "    # features MINIMAS pra ML sem depender de painel:\n",
    "    # usa Ã­ndice de tempo + mÃªs (do prÃ³prio perÃ­odo)\n",
    "    return y\n",
    "\n",
    "def pred_rf(period_index_train, y_train, period_index_pred):\n",
    "    # features: time + mÃªs + ano\n",
    "    X_train = pd.DataFrame({\n",
    "        \"time\": np.arange(len(period_index_train)),\n",
    "        \"mes\": period_index_train.month,\n",
    "        \"ano\": period_index_train.year\n",
    "    })\n",
    "    rf = RandomForestRegressor(n_estimators=400, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    X_pred = pd.DataFrame({\n",
    "        \"time\": np.arange(len(period_index_train), len(period_index_train) + len(period_index_pred)),\n",
    "        \"mes\": period_index_pred.month,\n",
    "        \"ano\": period_index_pred.year\n",
    "    })\n",
    "    return np.maximum(rf.predict(X_pred), 0)\n",
    "\n",
    "def pred_gb(period_index_train, y_train, period_index_pred):\n",
    "    X_train = pd.DataFrame({\n",
    "        \"time\": np.arange(len(period_index_train)),\n",
    "        \"mes\": period_index_train.month,\n",
    "        \"ano\": period_index_train.year\n",
    "    })\n",
    "    gb = GradientBoostingRegressor(random_state=42)\n",
    "    gb.fit(X_train, y_train)\n",
    "\n",
    "    X_pred = pd.DataFrame({\n",
    "        \"time\": np.arange(len(period_index_train), len(period_index_train) + len(period_index_pred)),\n",
    "        \"mes\": period_index_pred.month,\n",
    "        \"ano\": period_index_pred.year\n",
    "    })\n",
    "    return np.maximum(gb.predict(X_pred), 0)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) ESCOLHER MELHOR MODELO POR COD_PROD (backtest) + prever futuro\n",
    "# ============================================================\n",
    "\n",
    "JANELA_VALIDACAO = 12\n",
    "\n",
    "registros = []\n",
    "best_model_por_sku = {}  # guardar o modelo escolhido por COD_PROD (para backtest completo)\n",
    "\n",
    "total_skus = df_hist_base[\"COD_PROD\"].nunique()\n",
    "print(f\"ðŸš€ Iniciando previsÃ£o por SKU | Total: {total_skus:,}\")\n",
    "\n",
    "for i, (cod_prod, df_sku) in enumerate(df_hist_base.groupby(\"COD_PROD\"), start=1):\n",
    "\n",
    "    if i == 1 or i % 50 == 0:\n",
    "        print(f\"   â–¶ï¸ Processando SKU {i}/{total_skus} | COD_PROD={cod_prod}\")\n",
    "\n",
    "    df_sku = df_sku.sort_values(\"PERIODO\")\n",
    "    y = df_sku[\"VOL_VENDA\"].values.astype(float)\n",
    "    idx = pd.DatetimeIndex(df_sku[\"PERIODO\"])\n",
    "\n",
    "    # histÃ³rico muito curto -> LR\n",
    "    if len(y) < 6:\n",
    "        fc = pred_lr(y, horizon)\n",
    "        best = \"LinearRegression_Fallback\"\n",
    "        best_model_por_sku[cod_prod] = best\n",
    "        for per, val in zip(future_dates, fc):\n",
    "            registros.append([cod_prod, per, float(val), best])\n",
    "        continue\n",
    "\n",
    "    J = min(JANELA_VALIDACAO, max(3, len(y)//3))\n",
    "    y_train, y_val = y[:-J], y[-J:]\n",
    "    idx_train, idx_val = idx[:-J], idx[-J:]\n",
    "\n",
    "    scores = {}\n",
    "\n",
    "    # 1) HW\n",
    "    try:\n",
    "        pred_val = pred_hw(y_train, J)\n",
    "        scores[\"HoltWinters\"] = metric(y_val, pred_val)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) ARIMA\n",
    "    try:\n",
    "        pred_val = pred_arima(y_train, J)\n",
    "        scores[\"ARIMA\"] = metric(y_val, pred_val)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 3) LR\n",
    "    try:\n",
    "        pred_val = pred_lr(y_train, J)\n",
    "        scores[\"LinearRegression\"] = metric(y_val, pred_val)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 4) RF (ML)\n",
    "    try:\n",
    "        pred_val = pred_rf(idx_train, y_train, idx_val)\n",
    "        scores[\"RandomForest\"] = metric(y_val, pred_val)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 5) GB (ML)\n",
    "    try:\n",
    "        pred_val = pred_gb(idx_train, y_train, idx_val)\n",
    "        scores[\"GradientBoosting\"] = metric(y_val, pred_val)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # escolher melhor e treinar no histÃ³rico completo\n",
    "    if not scores:\n",
    "        best = \"LinearRegression_Fallback\"\n",
    "        fc = pred_lr(y, horizon)\n",
    "    else:\n",
    "        best = min(scores.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "        if best == \"HoltWinters\":\n",
    "            fc = pred_hw(y, horizon)\n",
    "        elif best == \"ARIMA\":\n",
    "            fc = pred_arima(y, horizon)\n",
    "        elif best == \"LinearRegression\":\n",
    "            fc = pred_lr(y, horizon)\n",
    "        elif best == \"RandomForest\":\n",
    "            fc = pred_rf(idx, y, future_dates)\n",
    "        else:  # GradientBoosting\n",
    "            fc = pred_gb(idx, y, future_dates)\n",
    "\n",
    "    best_model_por_sku[cod_prod] = best\n",
    "\n",
    "    for per, val in zip(future_dates, fc):\n",
    "        registros.append([cod_prod, per, float(val), best])\n",
    "\n",
    "print(\"ðŸ§© Finalizando consolidaÃ§Ã£o de resultados (forecast futuro)...\")\n",
    "\n",
    "df_forecast = pd.DataFrame(registros, columns=[\"COD_PROD\", \"PERIODO\", \"VOL_VENDA_REAL\", \"MODELO_ESCOLHIDO\"])\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) SAÃDA FINAL (histÃ³rico + projeÃ§Ã£o)\n",
    "# ============================================================\n",
    "\n",
    "df_final_hist = df_hist_base.rename(columns={\"VOL_VENDA\": \"VOL_VENDA_REAL\"}).copy()\n",
    "df_final_hist[\"MODELO_ESCOLHIDO\"] = np.nan\n",
    "\n",
    "df_forecast_estatistico_krona = pd.concat([df_final_hist, df_forecast], ignore_index=True)\n",
    "\n",
    "print(\"ðŸ“¦ Dataset base montado (histÃ³rico + futuro).\")\n",
    "\n",
    "# ============================================================\n",
    "# 6.1) BACKTEST COMPLETO (walk-forward) com o MESMO modelo do futuro\n",
    "#      Preenche o histÃ³rico inteiro (a partir de MIN_TREINO)\n",
    "# ============================================================\n",
    "\n",
    "print(\"ðŸ§ª Iniciando backtest completo (walk-forward) por SKU...\")\n",
    "\n",
    "STEP_BACKTEST = 6   # Treina a cada 6 meses\n",
    "\n",
    "# criar colunas novas (SEM criar APE_PCT / MAPE_SKU_PCT)\n",
    "df_forecast_estatistico_krona[\"PREVISAO_BACKTEST\"] = np.nan\n",
    "df_forecast_estatistico_krona[\"MODELO_BACKTEST\"] = np.nan\n",
    "df_forecast_estatistico_krona[\"APE\"] = np.nan\n",
    "df_forecast_estatistico_krona[\"MAPE_SKU\"] = np.nan\n",
    "\n",
    "MIN_TREINO = 12  # mÃ­nimo de meses para comeÃ§ar a prever o passado (walk-forward)\n",
    "\n",
    "# vamos trabalhar no histÃ³rico apenas (onde MODELO_ESCOLHIDO Ã© NaN)\n",
    "df_hist_all = df_forecast_estatistico_krona[df_forecast_estatistico_krona[\"MODELO_ESCOLHIDO\"].isna()].copy()\n",
    "df_hist_all = df_hist_all.sort_values([\"COD_PROD\", \"PERIODO\"])\n",
    "\n",
    "total_skus_bt = df_hist_all[\"COD_PROD\"].nunique()\n",
    "print(f\"ðŸ”Ž Backtest completo | Total SKUs: {total_skus_bt:,} | MIN_TREINO={MIN_TREINO}\")\n",
    "\n",
    "mape_por_sku_final = {}  # MAPE final calculado pelo backtest completo\n",
    "\n",
    "for i, (cod_prod, d) in enumerate(df_hist_all.groupby(\"COD_PROD\"), start=1):\n",
    "\n",
    "    if i == 1 or i % 50 == 0:\n",
    "        print(f\"   â–¶ï¸ Backtest SKU {i}/{total_skus_bt} | COD_PROD={cod_prod}\")\n",
    "\n",
    "    d = d.sort_values(\"PERIODO\")\n",
    "    y = d[\"VOL_VENDA_REAL\"].values.astype(float)\n",
    "    idx = pd.DatetimeIndex(d[\"PERIODO\"])\n",
    "\n",
    "    best = best_model_por_sku.get(cod_prod, \"LinearRegression_Fallback\")\n",
    "\n",
    "    preds = np.full(len(y), np.nan, dtype=float)\n",
    "    ape = np.full(len(y), np.nan, dtype=float)\n",
    "\n",
    "    t = MIN_TREINO\n",
    "\n",
    "    while t < len(y):\n",
    "        y_train = y[:t]\n",
    "        idx_train = idx[:t]\n",
    "\n",
    "        steps = min(STEP_BACKTEST, len(y) - t)\n",
    "        idx_pred = idx[t:t + steps]\n",
    "\n",
    "        try:\n",
    "            if best == \"HoltWinters\":\n",
    "                y_preds = pred_hw(y_train, steps)\n",
    "            elif best == \"ARIMA\":\n",
    "                y_preds = pred_arima(y_train, steps)\n",
    "            elif best in (\"LinearRegression\", \"LinearRegression_Fallback\"):\n",
    "                y_preds = pred_lr(y_train, steps)\n",
    "            elif best == \"RandomForest\":\n",
    "                y_preds = pred_rf(idx_train, y_train, idx_pred)\n",
    "            else:  # GradientBoosting\n",
    "                y_preds = pred_gb(idx_train, y_train, idx_pred)\n",
    "        except Exception:\n",
    "            y_preds = np.full(steps, np.mean(y_train) if len(y_train) else 0.0)\n",
    "\n",
    "        for i_step in range(steps):\n",
    "            pos = t + i_step\n",
    "            preds[pos] = max(float(y_preds[i_step]), 0)\n",
    "\n",
    "            if y[pos] != 0:\n",
    "                ape[pos] = abs((y[pos] - preds[pos]) / y[pos])\n",
    "\n",
    "        t += steps\n",
    "\n",
    "\n",
    "    mape_sku = float(np.nanmean(ape))\n",
    "    mape_por_sku_final[cod_prod] = mape_sku\n",
    "\n",
    "    # escrever de volta no df final (somente linhas histÃ³ricas do SKU)\n",
    "    mask = (df_forecast_estatistico_krona[\"COD_PROD\"] == cod_prod) & (df_forecast_estatistico_krona[\"MODELO_ESCOLHIDO\"].isna())\n",
    "\n",
    "    # garantir alinhamento: como d estÃ¡ em ordem e mask pega as linhas do SKU no mesmo bloco histÃ³rico,\n",
    "    # vamos atribuir pela ordem (via values) para evitar problemas com Ã­ndice\n",
    "    idx_rows = df_forecast_estatistico_krona.loc[mask].sort_values(\"PERIODO\").index\n",
    "\n",
    "    df_forecast_estatistico_krona.loc[idx_rows, \"PREVISAO_BACKTEST\"] = preds\n",
    "    df_forecast_estatistico_krona.loc[idx_rows, \"MODELO_BACKTEST\"] = best\n",
    "    df_forecast_estatistico_krona.loc[idx_rows, \"APE\"] = ape\n",
    "    df_forecast_estatistico_krona.loc[idx_rows, \"MAPE_SKU\"] = mape_sku\n",
    "\n",
    "# preencher MAPE_SKU tambÃ©m no futuro (indicador por SKU)\n",
    "df_forecast_estatistico_krona[\"MAPE_SKU\"] = df_forecast_estatistico_krona[\"MAPE_SKU\"].fillna(\n",
    "    df_forecast_estatistico_krona[\"COD_PROD\"].map(mape_por_sku_final)\n",
    ")\n",
    "\n",
    "# Criar coluna PREVISAO_FINAL baseada em PREVISAO_BACKTEST, porÃ©m nos periodos futuros usar VOL_VENDA_REAL\n",
    "df_forecast_estatistico_krona[\"PREVISAO_FINAL\"] = np.where(\n",
    "    df_forecast_estatistico_krona[\"PERIODO\"].isin(future_dates),\n",
    "    df_forecast_estatistico_krona[\"VOL_VENDA_REAL\"],\n",
    "    df_forecast_estatistico_krona[\"PREVISAO_BACKTEST\"]\n",
    ")\n",
    "\n",
    "print(\"âœ… Backtest completo concluÃ­do (histÃ³rico preenchido).\")\n",
    "\n",
    "# ============================================================\n",
    "# 7) SALVAR CSV\n",
    "# ============================================================\n",
    "\n",
    "df_forecast_estatistico_krona.to_csv(\n",
    "    pasta_staging_parquet / \"df_forecast_estatistico_krona.csv\",\n",
    "    sep=\";\",\n",
    "    encoding=\"utf-8-sig\",\n",
    "    index=False,\n",
    "    decimal=\",\",\n",
    "    float_format=\"%.2f\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Finalizado e salvo: df_forecast_estatistico_krona.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc9f8342",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "ExecuÃ§Ã£o interrompida DEBUG.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mparar_execucao\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\OneDrive\\BC\\03. Projetos Bedin\\01. Krona\\PREVISAO_DEMANDA\\PREV_DEMANDA\\00_SCRIPTS\\functions.py:82\u001b[39m, in \u001b[36mparar_execucao\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparar_execucao\u001b[39m():\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mExecuÃ§Ã£o interrompida DEBUG.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAssertionError\u001b[39m: ExecuÃ§Ã£o interrompida DEBUG."
     ]
    }
   ],
   "source": [
    "parar_execucao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "61f87121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Iniciando processo de desagregaÃ§Ã£o (histÃ³rico + futuro)...\n",
      "ðŸ“¦ df_vendas_krona em memÃ³ria | Linhas: 11.105.253\n",
      "ðŸ“Š Lendo arquivo de previsÃ£o estatÃ­stica salvo...\n",
      "ðŸ“Š Forecast carregado | SKUs: 1.180 | PerÃ­odos: 4\n",
      "ðŸ—“ï¸ Horizonte de previsÃ£o | Meses: 4 | 2026-03-01 â†’ 2026-06-01\n",
      "ðŸ“¦ Montando base df_prev_krona (histÃ³rico)...\n",
      "ðŸš€ Gerando percentuais de desagregaÃ§Ã£o\n",
      "ðŸš€ Aplicando percentuais para desagregar o forecast estatÃ­stico...\n",
      "âœ… df_prev_krona pronto | Linhas: 5.546.032\n",
      "ðŸ’¾ Salvando df_prev_krona completo em Parquet...\n",
      "âœ… Parquet salvo com sucesso: df_prev_krona.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# DESAGREGAÃ‡ÃƒO DO FORECAST ESTATÃSTICO (HISTÃ“RICO + FUTURO)\n",
    "# SAÃDA ÃšNICA: df_prev_krona.parquet (HIST + FUT)\n",
    "# ============================================================\n",
    "\n",
    "# =========================\n",
    "# START\n",
    "# =========================\n",
    "print(\"ðŸ”„ Iniciando processo de desagregaÃ§Ã£o (histÃ³rico + futuro)...\")\n",
    "print(f\"ðŸ“¦ df_vendas_krona em memÃ³ria | Linhas: {len(df_vendas_krona):,}\".replace(\",\", \".\"))\n",
    "\n",
    "# =========================\n",
    "# LER FORECAST\n",
    "# =========================\n",
    "print(\"ðŸ“Š Lendo arquivo de previsÃ£o estatÃ­stica salvo...\")\n",
    "\n",
    "arquivo_forecast = pasta_staging_parquet / \"df_forecast_estatistico_krona.csv\"\n",
    "\n",
    "df_forecast_estatistico_krona = pd.read_csv(\n",
    "    arquivo_forecast,\n",
    "    sep=\";\",\n",
    "    encoding=\"utf-8-sig\",\n",
    "    decimal=\",\",\n",
    "    dtype={\"COD_PROD\": str},\n",
    "    dayfirst=True\n",
    ")\n",
    "\n",
    "df_forecast_estatistico_krona[\"PERIODO\"] = pd.to_datetime(df_forecast_estatistico_krona[\"PERIODO\"])\n",
    "df_vendas_krona[\"PERIODO\"] = pd.to_datetime(df_vendas_krona[\"PERIODO\"])\n",
    "\n",
    "# Eliminar colunas desnecessÃ¡rias no df_forecast_estatistico_krona\n",
    "df_forecast_estatistico_krona = df_forecast_estatistico_krona.drop(\n",
    "    columns=[\"MODELO_ESCOLHIDO\", \"PREVISAO_BACKTEST\", \"MODELO_BACKTEST\", \"APE\", \"MAPE_SKU\", \"VOL_VENDA_REAL\"]\n",
    ")\n",
    "\n",
    "# Manter somente PERIODO conforme variÃ¡vel df_periodo_previsao\n",
    "df_forecast_estatistico_krona = df_forecast_estatistico_krona[\n",
    "    df_forecast_estatistico_krona[\"PERIODO\"].isin(\n",
    "        pd.to_datetime(df_periodo_previsao[\"PERIODO_PROJECAO\"].unique())\n",
    "    )\n",
    "].copy().reset_index(drop=True)\n",
    "\n",
    "n_skus = df_forecast_estatistico_krona[\"COD_PROD\"].nunique()\n",
    "n_periodos = df_forecast_estatistico_krona[\"PERIODO\"].nunique()\n",
    "p_min = df_forecast_estatistico_krona[\"PERIODO\"].min()\n",
    "p_max = df_forecast_estatistico_krona[\"PERIODO\"].max()\n",
    "\n",
    "print(f\"ðŸ“Š Forecast carregado | SKUs: {n_skus:,} | PerÃ­odos: {n_periodos}\".replace(\",\", \".\"))\n",
    "print(f\"ðŸ—“ï¸ Horizonte de previsÃ£o | Meses: {n_periodos} | {p_min.date()} â†’ {p_max.date()}\")\n",
    "\n",
    "# =========================\n",
    "# DEFINIR HISTÃ“RICO PARA DESAGREGAÃ‡ÃƒO DO FORECAST\n",
    "# =========================\n",
    "print(\"ðŸ“¦ Montando base df_prev_krona (histÃ³rico)...\")\n",
    "meses_hist_desagregacao = 12\n",
    "\n",
    "# df_prev_krona deve ser cÃ³pia de df_vendas_krona, filtrando PERIODO pela variavel meses_hist_desagregacao, retornar os ultimos 12 que constam no arquivo df_vendas_krona\n",
    "periodos_disponiveis = sorted(df_vendas_krona[\"PERIODO\"].unique())\n",
    "periodos_para_manter = periodos_disponiveis[-meses_hist_desagregacao:]\n",
    "df_prev_krona = df_vendas_krona[df_vendas_krona[\"PERIODO\"].isin(periodos_para_manter)].copy().reset_index(drop=True)\n",
    "\n",
    "# Agrupar dados somando as VOL_VENDA\n",
    "chaves_desagregacao = [\n",
    "    \"EMPRESA\",\"COD_CLIENTE\",\"NOME_CLIENTE\",\"COD_GRUPO_CLIENTE\",\"DESC_GRUPO_E_CLIENTE\",\n",
    "    \"COD_PROD\",\"DESC_PRODUTO\",\"FAMILIA\",\"LINHA\",\"REGIONAL\",\"REGIONAL_GESTOR\"\n",
    "]\n",
    "chaves_sem_periodo = chaves_desagregacao[:]\n",
    "df_prev_krona = df_prev_krona.groupby(\n",
    "    chaves_desagregacao,\n",
    "    as_index=False\n",
    ").agg({'VOL_VENDA': 'sum'}).reset_index(drop=True)\n",
    "\n",
    "print(\"ðŸš€ Gerando percentuais de desagregaÃ§Ã£o\")\n",
    "\n",
    "# Criar coluna TOTAL_VOL_VENDA por COD_PROD\n",
    "total_vol_venda_por_prod = df_prev_krona.groupby('COD_PROD')['VOL_VENDA'].transform('sum')\n",
    "df_prev_krona[\"TOTAL_VOL_VENDA\"] = total_vol_venda_por_prod\n",
    "\n",
    "# Criar coluna PERC_DESAGR\n",
    "df_prev_krona[\"PERC_DESAGR\"] = df_prev_krona[\"VOL_VENDA\"] / df_prev_krona[\"TOTAL_VOL_VENDA\"]\n",
    "\n",
    "df_prev_explodido = (\n",
    "    df_prev_krona\n",
    "    .merge(\n",
    "        df_forecast_estatistico_krona,\n",
    "        on=\"COD_PROD\",\n",
    "        how=\"inner\"   # sÃ³ explode onde existe forecast\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Aplicando percentuais para desagregar o forecast estatÃ­stico...\")\n",
    "\n",
    "df_prev_explodido[\"VOL_PREV\"] = (\n",
    "    df_prev_explodido[\"PREVISAO_FINAL\"] * df_prev_explodido[\"PERC_DESAGR\"]\n",
    ")\n",
    "\n",
    "df_prev_krona = df_prev_explodido.copy()\n",
    "\n",
    "# FIXME\n",
    "del df_prev_explodido, df_forecast_estatistico_krona, df_vendas_krona\n",
    "gc.collect()\n",
    "\n",
    "print(f\"âœ… df_prev_krona pronto | Linhas: {len(df_prev_krona):,}\".replace(\",\", \".\"))\n",
    "\n",
    "print(\"ðŸ’¾ Salvando df_prev_krona completo em Parquet...\")\n",
    "\n",
    "df_prev_krona.to_parquet(\n",
    "    pasta_staging_parquet / \"df_prev_krona.parquet\",\n",
    "    engine=\"pyarrow\",\n",
    "    compression=\"snappy\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"âœ… Parquet salvo com sucesso: df_prev_krona.parquet\")\n",
    "\n",
    "del df_prev_krona\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cea3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Gerando arquivos Forecast para Painel S&OP...\n",
      "âœ… Arquivo FORECAST_KRONA_AGREGADO.csv gerado com sucesso!\n",
      "âœ… Arquivo FORECAST_KRONA_CLIENTE.csv gerado com sucesso!\n",
      "\n",
      "â±ï¸ Tempo total de processamento: 368 min 23.8 s\n",
      "ðŸŽ¯ Processo concluÃ­do com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Separar a df_forecast_vendas_krona em dois dataframes:\n",
    "# df_forecast_vendas_krona_CLIENTE: clientes que terÃ£o planejamento de demanda\n",
    "# df_forecast_vendas_krona_PRODUTO: produtos que terÃ£o planejamento de demanda\n",
    "\n",
    "print(\"ðŸ”„ Gerando arquivos Forecast para Painel S&OP...\")\n",
    "\n",
    "# Ler df_prev_krona do Parquet\n",
    "df_prev_krona = pd.read_parquet(pasta_staging_parquet / \"df_prev_krona.parquet\")\n",
    "\n",
    "df_forecast_vendas_krona = df_prev_krona[\n",
    "    df_prev_krona[\"PERIODO\"].isin(df_periodo_previsao[\"PERIODO_PROJECAO\"])\n",
    "].copy()\n",
    "\n",
    "# Se lista_clientes_plan_demanda estiver vazio â†’ todos sÃ£o PRODUTO\n",
    "if lista_clientes_plan_demanda and len(lista_clientes_plan_demanda) > 0:\n",
    "    df_forecast_vendas_krona['NIVEL_PLAN_DEMANDA'] = np.where(\n",
    "        df_forecast_vendas_krona['COD_GRUPO_CLIENTE'].isin(lista_clientes_plan_demanda),\n",
    "        'CLIENTE',\n",
    "        'PRODUTO'\n",
    "    )\n",
    "else:\n",
    "    # Se nÃ£o existe cliente para plan. demanda â†’ tudo produto\n",
    "    df_forecast_vendas_krona['NIVEL_PLAN_DEMANDA'] = 'PRODUTO'\n",
    "    \n",
    "# Separar os dataframes com cÃ³pia explÃ­cita\n",
    "df_forecast_vendas_krona_CLIENTE = df_forecast_vendas_krona[df_forecast_vendas_krona['NIVEL_PLAN_DEMANDA'] == 'CLIENTE'].copy()\n",
    "df_forecast_vendas_krona_PRODUTO = df_forecast_vendas_krona[df_forecast_vendas_krona['NIVEL_PLAN_DEMANDA'] == 'PRODUTO'].copy()\n",
    "\n",
    "# Eliminar coluna NIVEL_PLAN_DEMANDA\n",
    "df_forecast_vendas_krona_CLIENTE.drop(columns=['NIVEL_PLAN_DEMANDA'], inplace=True)\n",
    "df_forecast_vendas_krona_CLIENTE.reset_index(drop=True, inplace=True)\n",
    "df_forecast_vendas_krona_PRODUTO.drop(columns=['NIVEL_PLAN_DEMANDA'], inplace=True)\n",
    "df_forecast_vendas_krona_PRODUTO.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#========================================================\n",
    "# PADRONIZANDO TEMPLATE df_forecast_vendas_krona_PRODUTO\n",
    "#========================================================\n",
    "\n",
    "# Eliminar colunas NOME_CLIENTE E DESC_GRUPO_E_CLIENTE\n",
    "df_forecast_vendas_krona_PRODUTO.drop(columns=['COD_CLIENTE', 'NOME_CLIENTE', 'COD_GRUPO_CLIENTE', 'DESC_GRUPO_E_CLIENTE'], inplace=True)\n",
    "\n",
    "# Sumarizar df_forecast_vendas_krona_PRODUTO por EMPRESA, COD_PROD, DESC_PRODUTO, FAMILIA, LINHA, REGIONAL, PERIODO\n",
    "df_forecast_vendas_krona_PRODUTO = df_forecast_vendas_krona_PRODUTO.groupby(\n",
    "    ['EMPRESA', 'COD_PROD', 'DESC_PRODUTO', 'FAMILIA', 'LINHA', 'REGIONAL', 'REGIONAL_GESTOR', 'PERIODO'],\n",
    "    as_index=False\n",
    ").agg({'VOL_PREV': 'sum'}).reset_index(drop=True)\n",
    "\n",
    "# Gerar arquivos em PARQUET\n",
    "df_forecast_vendas_krona_PRODUTO.to_parquet(pasta_staging_parquet / 'df_forecast_vendas_krona_PRODUTO.parquet', index=False)\n",
    "df_forecast_vendas_krona_CLIENTE.to_parquet(pasta_staging_parquet / 'df_forecast_vendas_krona_CLIENTE.parquet', index=False)\n",
    "\n",
    "# ðŸ“¤ ExportaÃ§Ã£o de Dados Forecast para Planejamento Colaborativo\n",
    "# ðŸ“Š NÃ­vel de agregaÃ§Ã£o: REGIONAL_GESTOR, 'REGIONAL', 'FAMILIA', 'PERIODO'\n",
    "\n",
    "df_Forecast_PRODUTO = df_forecast_vendas_krona_PRODUTO.groupby(\n",
    "    ['REGIONAL_GESTOR', 'REGIONAL', 'FAMILIA', 'PERIODO'],\n",
    "    as_index=False\n",
    ").agg({'VOL_PREV': 'sum'}).reset_index(drop=True)\n",
    "\n",
    "df_Forecast_PRODUTO.to_csv(\n",
    "    pasta_input_painel / 'FORECAST_KRONA_AGREGADO.csv',\n",
    "    sep=';',\n",
    "    encoding='utf-8-sig',\n",
    "    index=False,\n",
    "    decimal=',',\n",
    "    float_format=\"%.2f\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Arquivo FORECAST_KRONA_AGREGADO.csv gerado com sucesso!\")\n",
    "\n",
    "#========================================================\n",
    "# PADRONIZANDO TEMPLATE df_forecast_vendas_krona_CLIENTE\n",
    "#========================================================\n",
    "\n",
    "# ðŸ“¤ ExportaÃ§Ã£o de Dados Forecast para Planejamento Colaborativo\n",
    "# ðŸ“Š NÃ­vel de agregaÃ§Ã£o: REGIONAL_GESTOR, 'REGIONAL', 'COD_GRUPO_CLIENTE, DESC_GRUPO_E_CLIENTE, FAMILIA e PERIODO\n",
    "\n",
    "df_Forecast_CLIENTE = df_forecast_vendas_krona_CLIENTE.groupby(\n",
    "    ['REGIONAL_GESTOR', 'REGIONAL', 'COD_GRUPO_CLIENTE', 'DESC_GRUPO_E_CLIENTE', 'FAMILIA', 'PERIODO'],\n",
    "    as_index=False\n",
    ").agg({'VOL_PREV': 'sum'}).reset_index(drop=True)\n",
    "\n",
    "df_Forecast_CLIENTE.to_csv(\n",
    "    pasta_input_painel / 'FORECAST_KRONA_CLIENTE.csv',\n",
    "    sep=';',\n",
    "    encoding='utf-8-sig',\n",
    "    index=False,\n",
    "    decimal=',',\n",
    "    float_format=\"%.2f\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Arquivo FORECAST_KRONA_CLIENTE.csv gerado com sucesso!\")\n",
    "\n",
    "del df_forecast_vendas_krona_PRODUTO, df_forecast_vendas_krona_CLIENTE, lista_clientes_plan_demanda, df_prev_krona\n",
    "gc.collect()\n",
    "\n",
    "timer.finalizar()\n",
    "print(\"ðŸŽ¯ Processo concluÃ­do com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

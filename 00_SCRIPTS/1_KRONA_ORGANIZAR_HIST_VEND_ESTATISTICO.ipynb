{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa7d6663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Mapeamento de pastas concluÃ­do com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Importando bibliotecas\n",
    "from functions import *\n",
    "import pandas as pd\n",
    "import locale\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import duckdb\n",
    "import gc\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "import logging\n",
    "import shutil\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING, format='%(message)s')\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "timer = Temporizador()\n",
    "timer.iniciar()\n",
    "\n",
    "locale.setlocale(locale.LC_TIME, 'Portuguese_Brazil.1252')  # Para Windows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "# Detecta se o script estÃ¡ sendo executado de um .py ou de um notebook\n",
    "try:\n",
    "    caminho_base = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    # __file__ nÃ£o existe em Jupyter ou ambiente interativo\n",
    "    caminho_base = Path.cwd()\n",
    "\n",
    "pasta_input_parquet = caminho_base.parent / '01_INPUT_PIPELINE/01_BD_PARQUET'\n",
    "arquivo_input_regras_negocio = caminho_base.parent / '01_INPUT_PIPELINE/02_REGRAS_NEGOCIO/KRONA_REGRAS.xlsm'\n",
    "pasta_staging_parquet = caminho_base.parent / '02_STAGING_PARQUET' # Armazena arquivos parquet com tratamentos, aplicaÃ§Ãµes de regras, depara, etc\n",
    "pasta_input_painel = caminho_base.parent / '03_INPUT_PAINEL' # Armazena arquivos que serÃ£o consumidos no painel de S&OP para os gerentes\n",
    "pasta_painel = caminho_base.parent / '05_PAINEL'\n",
    "\n",
    "# Eliminar arquivos das pastas de 02_STAGING_PARQUET e 03_INPUT_PAINEL que serÃ£o regenerados\n",
    "pastas_para_limpar = [\n",
    "    pasta_staging_parquet,\n",
    "    pasta_input_painel,\n",
    "]\n",
    "\n",
    "for pasta in pastas_para_limpar:\n",
    "    if pasta.exists() and pasta.is_dir():\n",
    "        for item in pasta.iterdir():\n",
    "            if item.is_file() or item.is_symlink():\n",
    "                item.unlink()\n",
    "            elif item.is_dir():\n",
    "                shutil.rmtree(item)\n",
    "\n",
    "print(\"âœ… Mapeamento de pastas concluÃ­do com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b167ce4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ImportaÃ§Ã£o e tratamento de dados do arquivo KRONA_REGRAS, concluÃ­dos com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Carregar dados arquivo KRONA_REGRAS\n",
    "caminho_arquivo = arquivo_input_regras_negocio\n",
    "\n",
    "#-----------------------------------------------------------------------#\n",
    "#--------------- Carregar produtos eliminar ----------------------------#\n",
    "#-----------------------------------------------------------------------#\n",
    "guia_excel = 'PRODUTOS_ELIMINAR'\n",
    "df_produtos_eliminar = pd.read_excel(caminho_arquivo, sheet_name=guia_excel, engine='calamine', dtype={'COD_PROD': str})\n",
    "df_produtos_eliminar['COD_PROD'] = df_produtos_eliminar['COD_PROD'].astype(str)\n",
    "df_produtos_eliminar = df_produtos_eliminar.drop_duplicates(subset=['COD_PROD'])\n",
    "df_produtos_eliminar = df_produtos_eliminar[df_produtos_eliminar['COD_PROD'].notna()].reset_index(drop=True)\n",
    "\n",
    "#-----------------------------------------------------------------------#\n",
    "#---------------Carregar Regionais Gestor ------------------------------#\n",
    "#-----------------------------------------------------------------------#\n",
    "guia_excel = 'REGIONAIS_GESTOR'\n",
    "df_regionais_gestor = pd.read_excel(caminho_arquivo, sheet_name=guia_excel, engine='calamine')\n",
    "df_regionais_gestor = df_regionais_gestor.drop_duplicates(subset=['REGIONAL', 'REGIONAL_GESTOR'])\n",
    "df_regionais_gestor = df_regionais_gestor[df_regionais_gestor['REGIONAL'].notna()].reset_index(drop=True)\n",
    "\n",
    "#-----------------------------------------------------------------------#\n",
    "#---------------Carrregar Demanda LanÃ§amento Novos Produtos ------------#\n",
    "#-----------------------------------------------------------------------#\n",
    "guia_excel = 'PRODUTOS_LANCAMENTOS'\n",
    "df_produtos_lancamento = pd.read_excel(caminho_arquivo, sheet_name=guia_excel, engine='calamine')\n",
    "# ðŸš¨ VALIDAR SE EXISTEM DADOS\n",
    "if df_produtos_lancamento.empty:\n",
    "    raise ValueError(\n",
    "        \"âŒ ERRO: Nenhuma informaÃ§Ã£o foi encontrada na aba PRODUTOS_LANCAMENTOS.\\n\"\n",
    "        \"âž¡ï¸ Verifique se a planilha possui dados vÃ¡lidos antes de executar o pipeline.\"\n",
    "    )\n",
    "df_produtos_lancamento['JANELA LANÃ‡AMENTO'] = df_produtos_lancamento['JANELA LANÃ‡AMENTO'].astype(str).str.strip()\n",
    "df_produtos_lancamento = df_produtos_lancamento[df_produtos_lancamento['JANELA LANÃ‡AMENTO'] != ''].reset_index(drop=True)\n",
    "df_produtos_lancamento.rename(columns={'COD': 'COD_PROD'}, inplace=True)\n",
    "df_produtos_lancamento = df_produtos_lancamento[df_produtos_lancamento['COD_PROD'].notna()].reset_index(drop=True)\n",
    "df_produtos_lancamento['COD_PROD'] = df_produtos_lancamento['COD_PROD'].astype(str)\n",
    "\n",
    "# Identifica colunas com datas vÃ¡lidas\n",
    "col_datas = []\n",
    "for col in df_produtos_lancamento.columns:\n",
    "    try:\n",
    "        pd.to_datetime(col, dayfirst=True, errors='raise')\n",
    "        col_datas.append(col)\n",
    "    except (ValueError, TypeError):\n",
    "        continue\n",
    "\n",
    "colunas_validas = ['COD_PROD'] + \\\n",
    "                    [col for col in df_produtos_lancamento.columns if 'CD:' in str(col)] + \\\n",
    "                    col_datas\n",
    "df_produtos_lancamento = df_produtos_lancamento[[col for col in colunas_validas if col in df_produtos_lancamento.columns]]\n",
    "\n",
    "# Transforma datas em linhas\n",
    "df_produtos_lancamento = df_produtos_lancamento.melt(\n",
    "    id_vars=[col for col in df_produtos_lancamento.columns if col not in col_datas],\n",
    "    value_vars=col_datas,\n",
    "    var_name='PERIODO',\n",
    "    value_name='VALOR'\n",
    ")\n",
    "df_produtos_lancamento = df_produtos_lancamento[df_produtos_lancamento['VALOR'].notna()].reset_index(drop=True)\n",
    "\n",
    "# Multiplica colunas CD pelo VALOR\n",
    "colunas_cd = [col for col in df_produtos_lancamento.columns if 'CD:' in str(col)]\n",
    "for col in colunas_cd:\n",
    "    df_produtos_lancamento[col] = df_produtos_lancamento[col] * df_produtos_lancamento['VALOR']\n",
    "df_produtos_lancamento.drop(columns=['VALOR'], inplace=True)\n",
    "\n",
    "# Transforma colunas CD em linhas\n",
    "df_produtos_lancamento = df_produtos_lancamento.melt(\n",
    "    id_vars=[col for col in df_produtos_lancamento.columns if col not in colunas_cd],\n",
    "    value_vars=colunas_cd,\n",
    "    var_name='CD',\n",
    "    value_name='QTD'\n",
    ")\n",
    "\n",
    "#-----------------------------------------------------------------------#\n",
    "#---------------Carregar Regionais Construtora -------------------------#\n",
    "#-----------------------------------------------------------------------#\n",
    "guia_excel = 'REGIONAIS_CONSTRUTORA'\n",
    "df_regionais_construtora = pd.read_excel(caminho_arquivo, sheet_name=guia_excel, engine='calamine')\n",
    "df_regionais_construtora = df_regionais_construtora.drop_duplicates(subset=['REGIONAL BASE', 'REGIONAL ATUALIZADA'])\n",
    "\n",
    "#-----------------------------------------------------------------------#\n",
    "#---------------Carregar Clientes para planejamento de Demanda----------#\n",
    "#-----------------------------------------------------------------------#\n",
    "guia_excel = 'CLIENTES_DEMANDA'\n",
    "df_clientes_plan_demanda = pd.read_excel(caminho_arquivo, sheet_name=guia_excel, engine='calamine', dtype={'Cod_Grupo_Cliente': str})\n",
    "df_clientes_plan_demanda = df_clientes_plan_demanda.drop_duplicates(subset=['Cod_Grupo_Cliente'])\n",
    "df_clientes_plan_demanda = df_clientes_plan_demanda[df_clientes_plan_demanda['Cod_Grupo_Cliente'].notna()].reset_index(drop=True)\n",
    "\n",
    "# Converter a coluna de clientes para set para acelerar o isin\n",
    "lista_clientes_plan_demanda = set(df_clientes_plan_demanda['Cod_Grupo_Cliente'])\n",
    "\n",
    "# Unir COD_PROD de df_produtos_lancamento e df_produtos_eliminar, formar uma unica lista de produtos a eliminar, e remover do df_fato_vendas_krona\n",
    "# produtos_a_eliminar = pd.concat([df_produtos_eliminar[['COD_PROD']], df_produtos_lancamento[['COD_PROD']]]).drop_duplicates().reset_index(drop=True)\n",
    "# FIXME: Retirei os produtos de lanÃ§amento da lista de exclusÃ£o conforme solicitaÃ§Ã£o da Anna no WORD\n",
    "produtos_a_eliminar = df_produtos_eliminar[['COD_PROD']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "#-----------------------------------------------------------------------#\n",
    "#---------------Carregar DIRECIONA_CLIENTES_REGIONAL--------------------#\n",
    "#-----------------------------------------------------------------------#\n",
    "guia_excel = 'DIRECIONA_CLIENTES_REGIONAL'\n",
    "df_direc_cli_regional = pd.read_excel(caminho_arquivo, sheet_name=guia_excel, engine='calamine', dtype={'COD_GRUPO_CLIENTE': str, 'COD_CLIENTE': str})\n",
    "df_direc_cli_regional = df_direc_cli_regional[df_direc_cli_regional['COD_CLIENTE'].notna()].reset_index(drop=True)\n",
    "\n",
    "#-----------------------------------------------------------------------#\n",
    "#---------------Carregar PERIODO_PREVISAO-------------------------------#\n",
    "#-----------------------------------------------------------------------#\n",
    "guia_excel = 'PERIODO_PREVISAO'\n",
    "df_periodo_previsao = pd.read_excel(caminho_arquivo, sheet_name=guia_excel, engine='calamine')\n",
    "df_periodo_previsao = df_periodo_previsao[df_periodo_previsao['PERIODO_PROJECAO'].notna()].reset_index(drop=True)\n",
    "df_periodo_previsao = df_periodo_previsao.drop_duplicates(subset=['PERIODO_PROJECAO'])\n",
    "\n",
    "print(\"âœ… ImportaÃ§Ã£o e tratamento de dados do arquivo KRONA_REGRAS, concluÃ­dos com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54aed134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Script para eliminar duplicaÃ§Ã£o de Chv_Cliente no Dim_Clientes_Krona, conforme orientado por Marcos TI, criamos essa rotina para encontrar as duplicaÃ§Ãµes, eliminar e gerar novo Parquet sem duplicaÃ§Ãµes.\n",
    "\n",
    "# Carregar o Parquet\n",
    "df_dim_cli_krona = pd.read_parquet(pasta_input_parquet / \"Dim_Clientes_Krona.parquet\")\n",
    "\n",
    "# Eliminar duplciaÃ§Ãµes mantendo a primeira ocorrÃªncia\n",
    "df_dim_cli_krona = df_dim_cli_krona.drop_duplicates(subset=[\"Chv_Cliente\"], keep='first').reset_index(drop=True)\n",
    "\n",
    "# Gerar novo Parquet sem duplicaÃ§Ãµes\n",
    "df_dim_cli_krona.to_parquet(pasta_input_parquet / \"Dim_Clientes_Krona.parquet\", index=False)\n",
    "\n",
    "del df_dim_cli_krona\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019d269e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de8b62bf5c3f4da6b6b25782a5e1ff7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # FIXME Cosulta para validaÃ§Ã£o de dados Anna, direto do parquet Fato_Vendas_Krona.parquet, sem relacionamentos, agrupando por Cod_Produto, criando coluna mes pela Dat_Entrega_Venda no formato AAAAMM, somando Qtd_Venda\n",
    "\n",
    "# fato_vendas = (pasta_input_parquet / \"Fato_Vendas_Krona.parquet\").as_posix()\n",
    "\n",
    "# query = f\"\"\"\n",
    "# WITH base AS (\n",
    "#   SELECT\n",
    "#     Cod_Produto,\n",
    "#     Qtd_Venda,\n",
    "#     CAST(Dat_Entrega_Venda AS VARCHAR) AS dt_str\n",
    "#   FROM parquet_scan('{fato_vendas}')\n",
    "#   WHERE TRY_CAST(NULLIF(TRIM(Cod_Bloqueio), '') AS INTEGER) IN (80,90,95,99)\n",
    "#     AND Cod_Empresa IN ('01','05','08','0802','10')\n",
    "# ),\n",
    "# parse AS (\n",
    "#   SELECT\n",
    "#     Cod_Produto,\n",
    "#     Qtd_Venda,\n",
    "#     COALESCE(\n",
    "#       TRY_STRPTIME(dt_str, '%Y-%m-%d'),\n",
    "#       TRY_STRPTIME(dt_str, '%Y-%m-%d %H:%M:%S'),\n",
    "#       TRY_STRPTIME(dt_str, '%d/%m/%Y'),\n",
    "#       TRY_STRPTIME(dt_str, '%d/%m/%Y %H:%M:%S'),\n",
    "#       TRY_STRPTIME(dt_str, '%Y%m%d')\n",
    "#     )::DATE AS dt\n",
    "#   FROM base\n",
    "# )\n",
    "# SELECT\n",
    "#   Cod_Produto,\n",
    "#   STRFTIME(dt, '%Y%m') AS Mes,\n",
    "#   SUM(Qtd_Venda) AS Qtd_Venda_Total\n",
    "# FROM parse\n",
    "# WHERE dt IS NOT NULL\n",
    "# GROUP BY\n",
    "#   Cod_Produto,\n",
    "#   Mes\n",
    "# ORDER BY\n",
    "#   Cod_Produto,\n",
    "#   Mes\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# df_validacao_anna = duckdb.query(query).to_df()\n",
    "\n",
    "# # Gerar Excel para Anna\n",
    "# caminho_excel_saida = pasta_painel / f\"VALIDACAO_ANNA_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx\"\n",
    "# df_validacao_anna.to_excel(caminho_excel_saida, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8df03eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Query interrupted",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 163\u001b[39m\n\u001b[32m     26\u001b[39m duckdb.register(\u001b[33m\"\u001b[39m\u001b[33melim\u001b[39m\u001b[33m\"\u001b[39m, produtos_a_eliminar[[\u001b[33m'\u001b[39m\u001b[33mCOD_PROD\u001b[39m\u001b[33m'\u001b[39m]])\n\u001b[32m     28\u001b[39m sql = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[33mWITH\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[33mfato AS (\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    161\u001b[39m \u001b[33mFROM final\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m df_vendas_krona = \u001b[43mduckdb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ… Carregamento de dados concluÃ­do com sucesso!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Query interrupted"
     ]
    }
   ],
   "source": [
    "# Where Des_Origem = 'Krona'\n",
    "#   And Cod_Empresa IN ('01','05','08','0802','10')\n",
    "#   And Dat_Emissao_Venda >=Â '01/01/2018';\n",
    "\n",
    "# Na dimensÃ£o de Produto tem o seguinte filtro:\n",
    "# Where Cod_Empresa IN ('01','05','08','0802','10')\n",
    "\n",
    "# Nos casos de pedidos cancelados, consideramos apenas o que foi realmente vendido ao cliente:\n",
    "# Se o pedido foi cancelado por completo, nÃ£o aparece nenhuma venda.\n",
    "# Se apenas uma parte foi cancelada, consideramos somente a parte que foi vendida.\n",
    "# AlÃ©m disso, tambÃ©m existem os bloqueios de pedidos, que representam a â€œetapaâ€ em que o pedido se encontra. Nesses casos, Ã© importante definir quais bloqueios devem ser considerados nessa anÃ¡lise.\n",
    "\n",
    "# falei com o AndrÃ© aqui pelo chat, e verificou comÂ aÂ Aline\n",
    "# a principio utiliza os cÃ³digos de bloqueio 80, 90Â ,Â 95Â eÂ 99\n",
    "\n",
    "# No arquivo parquet, precisamos filtrar o campo Des_Origem = \"Krona\". O motivo Ã© que existem pedidos faturados pelo Protheus que contam no resultado da Viqua, e este aplicativo \"CML - Vendas\" do Qlik Sense traz apenas os pedidos que geram resultadoÂ paraÂ aÂ Krona\n",
    "\n",
    "# Conslidando as informaÃ§Ãµes de fontes em Parquet\n",
    "empresa = 'Krona'\n",
    "fact = (pasta_input_parquet / \"Fato_Vendas_Krona.parquet\").as_posix()\n",
    "prod = (pasta_input_parquet / \"Dim_Produtos_Vendas_Krona.parquet\").as_posix()\n",
    "cli  = (pasta_input_parquet / \"Dim_Clientes_Krona.parquet\").as_posix()\n",
    "vend = (pasta_input_parquet / \"Dim_Vendedores_Krona.parquet\").as_posix()\n",
    "\n",
    "# Eliminar produtos das listas em Excel: PRODUTOS ELIMINAR e PRODUTOS LANÃ‡AMENTOS\n",
    "duckdb.register(\"elim\", produtos_a_eliminar[['COD_PROD']])\n",
    "\n",
    "sql = f\"\"\"\n",
    "WITH\n",
    "fato AS (\n",
    "  SELECT\n",
    "    Cod_Produto,\n",
    "    Chv_Cliente,\n",
    "    Chv_Vendedor,\n",
    "    DATE_TRUNC(\n",
    "      'month',\n",
    "      CAST(\n",
    "        COALESCE(\n",
    "          TRY_STRPTIME(TRIM(Dat_Entrega_Venda), '%Y-%m-%d'),\n",
    "          TRY_STRPTIME(TRIM(Dat_Entrega_Venda), '%d/%m/%Y')\n",
    "        ) AS DATE\n",
    "      )\n",
    "    ) AS PERIODO,\n",
    "    TRIM(Nom_Empresa) AS EMPRESA,\n",
    "    SUM(TRY_CAST(Qtd_Venda AS DOUBLE)) AS QTD_VENDA,\n",
    "    SUM(TRY_CAST(Qtd_Peso_Venda AS DOUBLE)) AS VOL_VENDA\n",
    "  FROM parquet_scan('{fact}')\n",
    "  WHERE UPPER(TRIM(Nom_Empresa)) LIKE '%{empresa.strip().upper()}%'\n",
    "    AND UPPER(TRIM(Des_Origem))  LIKE '%{empresa.strip().upper()}%'\n",
    "    AND Cod_Empresa IN ('01','05','08','0802','10')\n",
    "    AND TRY_CAST(NULLIF(TRIM(Cod_Bloqueio), '') AS INTEGER) IN (80,90,95,99)\n",
    "    AND TRY_CAST(Qtd_Venda AS DOUBLE) > 0\n",
    "    AND Dat_Entrega_Venda IS NOT NULL\n",
    "    AND TRIM(Dat_Entrega_Venda) <> ''\n",
    "    AND COALESCE(\n",
    "      TRY_STRPTIME(TRIM(Dat_Entrega_Venda), '%Y-%m-%d'),\n",
    "      TRY_STRPTIME(TRIM(Dat_Entrega_Venda), '%d/%m/%Y')\n",
    "    ) >= DATE '2022-01-01'\n",
    "    -- EXCLUI MÃŠS ATUAL\n",
    "    AND DATE_TRUNC(\n",
    "      'month',\n",
    "      CAST(\n",
    "        COALESCE(\n",
    "          TRY_STRPTIME(TRIM(Dat_Entrega_Venda), '%Y-%m-%d'),\n",
    "          TRY_STRPTIME(TRIM(Dat_Entrega_Venda), '%d/%m/%Y')\n",
    "        ) AS DATE\n",
    "      )\n",
    "    ) < DATE_TRUNC('month', CURRENT_DATE)\n",
    "  GROUP BY Cod_Produto, Chv_Cliente, Chv_Vendedor, PERIODO, EMPRESA\n",
    "),\n",
    "prod AS (\n",
    "  SELECT\n",
    "    Cod_Produto,\n",
    "    TRIM(Des_Produto) AS Des_Produto,\n",
    "    Cod_Familia,\n",
    "    TRIM(Des_Familia) AS Des_Familia,\n",
    "    Cod_Linha,\n",
    "    TRIM(Des_Linha) AS Des_Linha,\n",
    "    TRIM(Nom_Empresa) AS EMPRESA,\n",
    "    TRY_CAST(Num_Peso AS DOUBLE) AS PESO_UNIT\n",
    "  FROM parquet_scan('{prod}')\n",
    "  WHERE Des_Linha IS NOT NULL\n",
    "    AND TRIM(Des_Linha) <> ''\n",
    "    AND Cod_Empresa IN ('01','05','08','0802','10')\n",
    "),\n",
    "cli AS (\n",
    "  SELECT\n",
    "    Chv_Cliente,\n",
    "    TRIM(Nom_Cliente) AS NOME_CLIENTE,\n",
    "    TRIM(Nom_Empresa) AS EMPRESA,\n",
    "    Chv_Vendedor_Cliente,\n",
    "    TRIM(Des_Segmento) AS SEGMENTO,\n",
    "    -- Corrige COD_GRUPO_CLIENTE: se vazio, usa COD_CLIENTE\n",
    "    CASE\n",
    "      WHEN TRIM(Cod_Grupo_Cliente) = '' OR Cod_Grupo_Cliente IS NULL\n",
    "      THEN TRIM(SPLIT_PART(Chv_Cliente, '|', 2))\n",
    "      ELSE TRIM(Cod_Grupo_Cliente)\n",
    "    END AS COD_GRUPO_CLIENTE,\n",
    "    -- Corrige DESC_GRUPO_E_CLIENTE: se vazio, usa NOME_CLIENTE\n",
    "    CASE\n",
    "      WHEN TRIM(Des_Grupo_e_Cliente) = '' OR Des_Grupo_e_Cliente IS NULL\n",
    "      THEN TRIM(Nom_Cliente)\n",
    "      ELSE TRIM(Des_Grupo_e_Cliente)\n",
    "    END AS DESC_GRUPO_E_CLIENTE\n",
    "  FROM parquet_scan('{cli}')\n",
    "),\n",
    "vend AS (\n",
    "  SELECT\n",
    "    Chv_Vendedor,\n",
    "    TRIM(Des_Regiao) AS Des_Regiao\n",
    "  FROM parquet_scan('{vend}')\n",
    "),\n",
    "base AS (\n",
    "  SELECT f.*\n",
    "  FROM fato f\n",
    "  WHERE NOT EXISTS (\n",
    "    SELECT 1 FROM elim e WHERE e.COD_PROD = f.Cod_Produto\n",
    "  )\n",
    "),\n",
    "final AS (\n",
    "  SELECT\n",
    "    b.EMPRESA,\n",
    "    TRIM(SPLIT_PART(c.Chv_Cliente, '|', 2)) AS COD_CLIENTE,\n",
    "    c.NOME_CLIENTE,\n",
    "    c.COD_GRUPO_CLIENTE,\n",
    "    c.DESC_GRUPO_E_CLIENTE,\n",
    "    c.SEGMENTO,\n",
    "    b.Cod_Produto AS COD_PROD,\n",
    "    p.Des_Produto AS DESC_PRODUTO,\n",
    "    CAST(p.Cod_Familia AS VARCHAR) || ' - ' || p.Des_Familia AS FAMILIA,\n",
    "    CAST(p.Cod_Linha   AS VARCHAR) || ' - ' || p.Des_Linha   AS LINHA,\n",
    "    v1.Des_Regiao AS REGIAO_CLIENTE,\n",
    "    v2.Des_Regiao AS REGIAO_MOVIMENTO,\n",
    "    b.PERIODO,\n",
    "    b.QTD_VENDA,\n",
    "    b.VOL_VENDA,\n",
    "    b.VOL_VENDA / b.QTD_VENDA AS PESO_UNIT\n",
    "  FROM base b\n",
    "  LEFT JOIN prod p ON b.Cod_Produto = p.Cod_Produto AND b.EMPRESA = p.EMPRESA\n",
    "  LEFT JOIN cli  c ON b.Chv_Cliente = c.Chv_Cliente AND b.EMPRESA = c.EMPRESA\n",
    "  LEFT JOIN vend v1 ON c.Chv_Vendedor_Cliente = v1.Chv_Vendedor\n",
    "  LEFT JOIN vend v2 ON b.Chv_Vendedor         = v2.Chv_Vendedor\n",
    ")\n",
    "SELECT\n",
    "  UPPER(EMPRESA) AS EMPRESA,\n",
    "  COD_CLIENTE,\n",
    "  NOME_CLIENTE,\n",
    "  COD_GRUPO_CLIENTE,\n",
    "  DESC_GRUPO_E_CLIENTE,\n",
    "  SEGMENTO,\n",
    "  COD_PROD,\n",
    "  DESC_PRODUTO,\n",
    "  FAMILIA,\n",
    "  LINHA,\n",
    "  PESO_UNIT,\n",
    "  REGIAO_CLIENTE,\n",
    "  REGIAO_MOVIMENTO,\n",
    "  PERIODO,\n",
    "  QTD_VENDA,\n",
    "  VOL_VENDA\n",
    "FROM final\n",
    "\"\"\"\n",
    "df_vendas_krona = duckdb.query(sql).to_df()\n",
    "\n",
    "print(\"âœ… Carregamento de dados concluÃ­do com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f4d87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc7d11cfd6d24127beb2aa23e5e3bca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OrganizaÃ§Ã£o de Regionais e InserÃ§Ã£o de Regional Gestor concluÃ­dos com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1. Criando coluna REGIONAL copiando a coluna REGIAO_CLIENTE \n",
    "#    no df_vendas_krona. \n",
    "#    Onde o segmento contÃ©m CONSTRUTORA ou INSTALADOR, buscar \n",
    "#    na tabela de regionais_construtora a regional atualizada.\n",
    "# ============================================================\n",
    "\n",
    "# Cria a tabela de de-para das regionais (jÃ¡ registrada no engine)\n",
    "duckdb.register(\"vendas\", df_vendas_krona)\n",
    "duckdb.register(\"map_reg\", df_regionais_construtora[['REGIONAL BASE','REGIONAL ATUALIZADA']])\n",
    "\n",
    "sql = \"\"\"\n",
    "WITH base AS (\n",
    "  SELECT\n",
    "    v.*,\n",
    "    -- Substitui valores vazios de REGIAO_CLIENTE por REGIAO_MOVIMENTO\n",
    "    COALESCE(NULLIF(v.REGIAO_CLIENTE,''), v.REGIAO_MOVIMENTO) AS RC_FIX,\n",
    "    UPPER(v.SEGMENTO) AS SEG_UP,\n",
    "    UPPER(v.REGIAO_CLIENTE) AS RC,\n",
    "    UPPER(v.REGIAO_MOVIMENTO) AS RM\n",
    "  FROM vendas v\n",
    "),\n",
    "\n",
    "ajuste AS (\n",
    "  SELECT\n",
    "    b.*,\n",
    "    CASE\n",
    "      -- 1) Se SEGMENTO contÃ©m CONSTRUTORA ou INSTALADOR => usa de-para\n",
    "      WHEN b.SEG_UP LIKE '%CONSTRUTORA%' OR b.SEG_UP LIKE '%INSTALADOR%'\n",
    "        THEN COALESCE(m.\"REGIONAL ATUALIZADA\", b.RC_FIX)\n",
    "      -- ============================================================\n",
    "      -- 2. Converter TELEVENDAS - Regras para definir REGIONAL:\n",
    "      --    REGIONAL = CONSTRUTORA => REGIONAL_CONSTRUTORA\n",
    "      --    REGIAO_CLIENTE = TELEVENDAS e REGIAO_MOVIMENTO = TELEVENDAS => TELEVENDAS\n",
    "      --    REGIAO_CLIENTE != TELEVENDAS e REGIAO_MOVIMENTO = TELEVENDAS => TELEVENDAS\n",
    "      --    REGIAO_CLIENTE = TELEVENDAS e REGIAO_MOVIMENTO != TELEVENDAS => REGIAO_MOVIMENTO\n",
    "      --    Caso contrÃ¡rio => REGIAO_CLIENTE\n",
    "      -- ============================================================\n",
    "      WHEN b.RC='TELEVENDAS' AND b.RM='TELEVENDAS' THEN 'TELEVENDAS'\n",
    "      WHEN b.RC<>'TELEVENDAS' AND b.RM='TELEVENDAS' THEN 'TELEVENDAS'\n",
    "      WHEN b.RC='TELEVENDAS' AND b.RM<>'TELEVENDAS' THEN b.RM\n",
    "      ELSE b.RC_FIX\n",
    "    END AS REGIONAL\n",
    "  FROM base b\n",
    "  LEFT JOIN map_reg m\n",
    "    ON m.\"REGIONAL BASE\" = b.REGIAO_CLIENTE\n",
    ")\n",
    "\n",
    "-- ============================================================\n",
    "-- Resultado final consolidado\n",
    "-- ============================================================\n",
    "SELECT\n",
    "  EMPRESA,\n",
    "  COD_CLIENTE,\n",
    "  NOME_CLIENTE,\n",
    "  COD_GRUPO_CLIENTE,\n",
    "  DESC_GRUPO_E_CLIENTE,\n",
    "  COD_PROD,\n",
    "  DESC_PRODUTO,\n",
    "  FAMILIA,\n",
    "  LINHA,\n",
    "  REGIONAL,\n",
    "  PERIODO,\n",
    "  SUM(QTD_VENDA) AS QTD_VENDA,\n",
    "  SUM(VOL_VENDA) AS VOL_VENDA\n",
    "FROM ajuste\n",
    "WHERE REGIONAL IS NOT NULL AND REGIONAL <> ''\n",
    "GROUP BY\n",
    "  EMPRESA,\n",
    "  COD_CLIENTE,\n",
    "  NOME_CLIENTE,\n",
    "  COD_GRUPO_CLIENTE,\n",
    "  DESC_GRUPO_E_CLIENTE,\n",
    "  COD_PROD,\n",
    "  DESC_PRODUTO,\n",
    "  FAMILIA,\n",
    "  LINHA,\n",
    "  REGIONAL,\n",
    "  PERIODO\n",
    "\"\"\"\n",
    "\n",
    "# Executa no DuckDB\n",
    "df_vendas_krona = duckdb.query(sql).to_df()\n",
    "\n",
    "# Inserir REGIONAL_GESTOR no df_vendas_krona\n",
    "df_vendas_krona = pd.merge(\n",
    "    df_vendas_krona,\n",
    "    df_regionais_gestor,\n",
    "    left_on='REGIONAL',\n",
    "    right_on='REGIONAL',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "colunas_ordenadas = [\n",
    "    \"EMPRESA\",\n",
    "    \"COD_CLIENTE\",\n",
    "    \"NOME_CLIENTE\",\n",
    "    \"COD_GRUPO_CLIENTE\",\n",
    "    \"DESC_GRUPO_E_CLIENTE\",\n",
    "    \"COD_PROD\",\n",
    "    \"DESC_PRODUTO\",\n",
    "    \"FAMILIA\",\n",
    "    \"LINHA\",\n",
    "    \"REGIONAL\",\n",
    "    \"REGIONAL_GESTOR\",\n",
    "    \"PERIODO\",\n",
    "    \"QTD_VENDA\",\n",
    "    \"VOL_VENDA\"\n",
    "]\n",
    "\n",
    "df_vendas_krona = df_vendas_krona[colunas_ordenadas]\n",
    "\n",
    "# Salvar df_vendas_krona em Parquet para salvar as alteraÃ§Ãµes, filtros e regras aplicadas no histÃ³rico, otimizando memÃ³ria e garantindo rastreabilidade\n",
    "df_vendas_krona.to_parquet(pasta_staging_parquet / \"df_vendas_krona.parquet\", index=False)\n",
    "\n",
    "print(\"âœ… OrganizaÃ§Ã£o de Regionais e InserÃ§Ã£o de Regional Gestor concluÃ­dos com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd99a756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ¦† ExportaÃ§Ã£o de Dados Vendas para Planejamento Colaborativo\n",
    "# ðŸŽ¯ Objetivo: Exportar CSV para o Plano Colaborativo\n",
    "df_vendas_krona['NIVEL_PLAN_DEMANDA'] = np.where(\n",
    "    df_vendas_krona['COD_GRUPO_CLIENTE'].isin(lista_clientes_plan_demanda),\n",
    "    'CLIENTE',\n",
    "    'PRODUTO'\n",
    ")\n",
    "\n",
    "# Separa os DataFrames\n",
    "df_hist_vend_PRODUTO = df_vendas_krona[df_vendas_krona['NIVEL_PLAN_DEMANDA'] == 'PRODUTO']\n",
    "df_hist_vend_CLIENTE = df_vendas_krona[df_vendas_krona['NIVEL_PLAN_DEMANDA'] == 'CLIENTE']\n",
    "\n",
    "# Eliminar coluna NIVEL_PLAN_DEMANDA\n",
    "df_hist_vend_PRODUTO = df_hist_vend_PRODUTO.drop(columns=['NIVEL_PLAN_DEMANDA'])\n",
    "df_hist_vend_CLIENTE = df_hist_vend_CLIENTE.drop(columns=['NIVEL_PLAN_DEMANDA'])\n",
    "\n",
    "# Agrupar df_hist_vend_PRODUTO por REGIONAL_GESTOR, FAMILIA, PERIODO, VOL_VENDA\n",
    "df_hist_vend_PRODUTO = df_hist_vend_PRODUTO.groupby(\n",
    "    ['REGIONAL_GESTOR', 'REGIONAL', 'FAMILIA', 'PERIODO'],\n",
    "    as_index=False\n",
    ").agg({'VOL_VENDA': 'sum'}).reset_index(drop=True)\n",
    "\n",
    "# Salva como CSV\n",
    "df_hist_vend_PRODUTO.to_csv(\n",
    "    pasta_input_painel / 'HIST_VENDA_KRONA_AGREGADO.csv',\n",
    "    sep=';',\n",
    "    encoding='utf-8-sig',\n",
    "    index=False,\n",
    "    decimal=',',\n",
    "    float_format=\"%.2f\"\n",
    ")\n",
    "\n",
    "# Agrupar df_hist_vend_CLIENTE por COD_GRUPO_CLIENTE, DESC_GRUPO_E_CLIENTE, REGIONAL_GESTOR, FAMILIA, PERIODO, VOL_VENDA\n",
    "df_hist_vend_CLIENTE = df_hist_vend_CLIENTE.groupby(\n",
    "    [\"COD_GRUPO_CLIENTE\",\"DESC_GRUPO_E_CLIENTE\", \"REGIONAL_GESTOR\", 'REGIONAL', \"FAMILIA\", \"PERIODO\"],\n",
    "    as_index=False\n",
    ").agg({'VOL_VENDA': 'sum'}).reset_index(drop=True)\n",
    "\n",
    "# Salva como CSV\n",
    "df_hist_vend_CLIENTE.to_csv(\n",
    "    pasta_input_painel / 'HIST_VENDA_KRONA_CLIENTE.csv',\n",
    "    sep=';',\n",
    "    encoding='utf-8-sig',\n",
    "    index=False,\n",
    "    decimal=',',\n",
    "    float_format=\"%.2f\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfb9c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar os arquivos com mÃ©dia de vendas para Planejamento Colaborativo Agregado\n",
    "# Encontrar o primeiro dia do mÃªs atual\n",
    "\n",
    "colunas_agregadas = ['REGIONAL_GESTOR', 'REGIONAL', 'FAMILIA']\n",
    "\n",
    "hoje = datetime.today()\n",
    "primeiro_dia_mes_atual = datetime(hoje.year, hoje.month, 1)\n",
    "\n",
    "# Calcular o primeiro dia do mÃªs de 6 meses atrÃ¡s (excluindo mÃªs atual)\n",
    "primeiro_dia_6_meses_atras = (primeiro_dia_mes_atual - pd.DateOffset(months=6)).to_pydatetime()\n",
    "\n",
    "# Filtrar apenas os Ãºltimos 6 meses (excluindo mÃªs atual)\n",
    "mask = (df_hist_vend_PRODUTO['PERIODO'] >= primeiro_dia_6_meses_atras) & (df_hist_vend_PRODUTO['PERIODO'] < primeiro_dia_mes_atual)\n",
    "df_hist_vend_PRODUTO_ultimos_6_meses = df_hist_vend_PRODUTO.loc[mask].copy()\n",
    "\n",
    "# Ordenar por data crescente\n",
    "df_hist_vend_PRODUTO_ultimos_6_meses = df_hist_vend_PRODUTO_ultimos_6_meses.sort_values('PERIODO').reset_index(drop=True)\n",
    "\n",
    "# DataFrame dos 3 meses mais recentes (Ãºltimos 3 meses do intervalo filtrado)\n",
    "df_3_meses_mais_recentes = df_hist_vend_PRODUTO_ultimos_6_meses.copy()\n",
    "\n",
    "# Identificar as 3 datas mais recentes (sem duplicar por linha)\n",
    "meses_recentes = sorted(df_3_meses_mais_recentes['PERIODO'].unique())[-3:]\n",
    "\n",
    "# Filtrar todas as linhas que pertencem a esses 3 meses\n",
    "df_3_meses_mais_recentes = df_3_meses_mais_recentes[df_3_meses_mais_recentes['PERIODO'].isin(meses_recentes)].copy()\n",
    "\n",
    "# Agrupa pelas colunas desejadas e calcula a mÃ©dia das colunas numÃ©ricas\n",
    "df_3_meses_mais_recentes_media = df_3_meses_mais_recentes.groupby(colunas_agregadas).mean(numeric_only=True).reset_index()\n",
    "\n",
    "# Adicionar coluna MEDIA informando 'MÃ‰DIA 3 MESES' na coluna\n",
    "df_3_meses_mais_recentes_media['MEDIA'] = 'MÃ‰DIA 3 MESES'\n",
    "\n",
    "# Agrupamento fazendo mÃ©dia dos 6 meses\n",
    "df_6_meses_mais_recentes_media = df_hist_vend_PRODUTO_ultimos_6_meses.copy()\n",
    "df_6_meses_mais_recentes_media = df_6_meses_mais_recentes_media.groupby(colunas_agregadas).mean(numeric_only=True).reset_index()\n",
    "\n",
    "# Adicionar coluna MEDIA informando 'MÃ‰DIA 6 MESES' na coluna\n",
    "df_6_meses_mais_recentes_media['MEDIA'] = 'MÃ‰DIA 6 MESES'\n",
    "\n",
    "# Concatenar os DataFrames\n",
    "df_media_vendas_PRODUTO = pd.concat([df_3_meses_mais_recentes_media, df_6_meses_mais_recentes_media], ignore_index=True)\n",
    "\n",
    "# Pivotar a coluna MEDIA\n",
    "df_media_vendas_PRODUTO = df_media_vendas_PRODUTO.pivot_table(\n",
    "    index=colunas_agregadas,\n",
    "    columns='MEDIA',\n",
    "    values='VOL_VENDA',\n",
    "    aggfunc='sum',\n",
    "    fill_value=0\n",
    ").reset_index()\n",
    "\n",
    "# Gerar o arquivo CSV\n",
    "df_media_vendas_PRODUTO.to_csv(\n",
    "    pasta_input_painel / 'MEDIA_VENDA_KRONA_AGREGADO.csv',\n",
    "    sep=';',\n",
    "    encoding='utf-8-sig',\n",
    "    index=False,\n",
    "    decimal=',',\n",
    "    float_format=\"%.2f\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfd335e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Bases de Vendas para Planejamento Colaborativo geradas com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Gerar os arquivos com mÃ©dia de vendas para Planejamento Colaborativo por Cliente\n",
    "# Encontrar o primeiro dia do mÃªs atual\n",
    "colunas_agrupadas = ['COD_GRUPO_CLIENTE', 'DESC_GRUPO_E_CLIENTE', 'REGIONAL_GESTOR', 'REGIONAL', 'FAMILIA']\n",
    "\n",
    "hoje = datetime.today()\n",
    "primeiro_dia_mes_atual = datetime(hoje.year, hoje.month, 1)\n",
    "\n",
    "# Calcular o primeiro dia do mÃªs de 6 meses atrÃ¡s (excluindo mÃªs atual)\n",
    "primeiro_dia_6_meses_atras = (primeiro_dia_mes_atual - pd.DateOffset(months=6)).to_pydatetime()\n",
    "\n",
    "# Filtrar apenas os Ãºltimos 6 meses (excluindo mÃªs atual)\n",
    "mask = (df_hist_vend_CLIENTE['PERIODO'] >= primeiro_dia_6_meses_atras) & (df_hist_vend_CLIENTE['PERIODO'] < primeiro_dia_mes_atual)\n",
    "df_hist_vend_CLIENTE_ultimos_6_meses = df_hist_vend_CLIENTE.loc[mask].copy()\n",
    "\n",
    "# Ordenar por data crescente\n",
    "df_hist_vend_CLIENTE_ultimos_6_meses = df_hist_vend_CLIENTE_ultimos_6_meses.sort_values('PERIODO').reset_index(drop=True)\n",
    "\n",
    "# DataFrame dos 3 meses mais recentes (Ãºltimos 3 meses do intervalo filtrado)\n",
    "df_3_meses_mais_recentes = df_hist_vend_CLIENTE_ultimos_6_meses.copy()\n",
    "\n",
    "# Identificar as 3 datas mais recentes (sem duplicar por linha)\n",
    "meses_recentes = sorted(df_3_meses_mais_recentes['PERIODO'].unique())[-3:]\n",
    "\n",
    "# Filtrar todas as linhas que pertencem a esses 3 meses\n",
    "df_3_meses_mais_recentes = df_3_meses_mais_recentes[df_3_meses_mais_recentes['PERIODO'].isin(meses_recentes)].copy()\n",
    "\n",
    "# Agrupa pelas colunas desejadas e calcula a mÃ©dia das colunas numÃ©ricas\n",
    "df_3_meses_mais_recentes_media = df_3_meses_mais_recentes.groupby(colunas_agrupadas).mean(numeric_only=True).reset_index()\n",
    "\n",
    "# Adicionar coluna MEDIA informando 'MÃ‰DIA 3 MESES' na coluna\n",
    "df_3_meses_mais_recentes_media['MEDIA'] = 'MÃ‰DIA 3 MESES'\n",
    "\n",
    "# Agrupamento fazendo mÃ©dia dos 6 meses\n",
    "df_6_meses_mais_recentes_media = df_hist_vend_CLIENTE_ultimos_6_meses.copy()\n",
    "df_6_meses_mais_recentes_media = df_6_meses_mais_recentes_media.groupby(colunas_agrupadas).mean(numeric_only=True).reset_index()\n",
    "\n",
    "# Adicionar coluna MEDIA informando 'MÃ‰DIA 6 MESES' na coluna\n",
    "df_6_meses_mais_recentes_media['MEDIA'] = 'MÃ‰DIA 6 MESES'\n",
    "\n",
    "# Concatenar os DataFrames\n",
    "df_media_vendas_PRODUTO = pd.concat([df_3_meses_mais_recentes_media, df_6_meses_mais_recentes_media], ignore_index=True)\n",
    "\n",
    "# Pivotar a coluna MEDIA\n",
    "df_media_vendas_PRODUTO = df_media_vendas_PRODUTO.pivot_table(\n",
    "    index=colunas_agrupadas,\n",
    "    columns='MEDIA',\n",
    "    values='VOL_VENDA',\n",
    "    aggfunc='sum',\n",
    "    fill_value=0\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "# Gerar o arquivo CSV\n",
    "df_media_vendas_PRODUTO.to_csv(\n",
    "    pasta_input_painel / 'MEDIA_VENDA_KRONA_CLIENTE.csv',\n",
    "    sep=';',\n",
    "    encoding='utf-8-sig',\n",
    "    index=False,\n",
    "    decimal=',',\n",
    "    float_format=\"%.2f\"\n",
    ")\n",
    "\n",
    "# FIXME\n",
    "del df_hist_vend_PRODUTO, df_hist_vend_CLIENTE, df_vendas_krona, produtos_a_eliminar\n",
    "gc.collect()\n",
    "\n",
    "print(\"âœ… Bases de Vendas para Planejamento Colaborativo geradas com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf44530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nðŸ“Š Modelos de PrevisÃ£o Aplicados (VersÃ£o Final)\\n\\nEste script aplica cinco modelos distintos de previsÃ£o de sÃ©ries temporais, compara o desempenho com base no RMSE\\n(Root Mean Squared Error) e seleciona automaticamente o mais assertivo para gerar a projeÃ§Ã£o final.\\n\\nModelos utilizados:\\n\\n1. RegressÃ£o Linear\\n   - Captura: tendÃªncias lineares de longo prazo.\\n   - Uso: quando o histÃ³rico mostra crescimento ou queda estÃ¡vel.\\n   - ForÃ§a: fÃ¡cil de justificar e visualizar; ideal para projeÃ§Ãµes simples e diretas.\\n\\n2. Holt-Winters (SuavizaÃ§Ã£o Exponencial com sazonalidade multiplicativa)\\n   - Captura: padrÃµes sazonais e tendÃªncia, com maior peso para os dados mais recentes.\\n   - Uso: quando hÃ¡ sazonalidade clara e variaÃ§Ãµes proporcionais ao volume.\\n   - ForÃ§a: modelo clÃ¡ssico, confiÃ¡vel e com excelente desempenho em sÃ©ries temporais mensais.\\n\\n3. ARIMA (AutoRegressive Integrated Moving Average)\\n   - Captura: dependÃªncia temporal e ruÃ­do estatÃ­stico, sem necessidade de sazonalidade explÃ­cita.\\n   - Uso: quando hÃ¡ padrÃ£o autoregressivo e estabilidade sem sazonalidade forte.\\n   - ForÃ§a: modelo estatÃ­stico robusto, ideal para sÃ©ries estacionÃ¡rias ou suavizadas.\\n\\n4. Random Forest Regressor com variÃ¡veis sazonais\\n   - Captura: relaÃ§Ãµes nÃ£o lineares e interaÃ§Ãµes entre tempo, mÃªs e ano.\\n   - Uso: quando hÃ¡ sazonalidade, mas o padrÃ£o nÃ£o Ã© linear nem estÃ¡vel.\\n   - ForÃ§a: flexÃ­vel, adaptÃ¡vel e resistente a ruÃ­dos; Ã³timo para sÃ©ries com comportamento irregular.\\n\\n5. Prophet (Facebook) com sazonalidade anual\\n   - Captura: tendÃªncia, sazonalidade e feriados (se configurado).\\n   - Uso: quando hÃ¡ sazonalidade anual bem definida e histÃ³rico suficiente.\\n   - ForÃ§a: fÃ¡cil de ajustar, escalÃ¡vel e excelente para previsÃµes com mÃºltiplos componentes.\\n\\nO modelo com menor RMSE nos Ãºltimos 12 meses Ã© selecionado automaticamente para gerar a previsÃ£o final,\\nque Ã© incorporada Ã  coluna 'VOL_VENDA_REAL' junto ao histÃ³rico, com marcaÃ§Ã£o do modelo escolhido.\\n\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "ðŸ§© HistÃ³rico dos Modelos Testados no Projeto\n",
    "--------------------------------------------\n",
    "\n",
    "1. MÃ©dia Simples / MÃ©dia 12M\n",
    "   DescriÃ§Ã£o: cÃ¡lculo da mÃ©dia das vendas dos Ãºltimos 12 meses.\n",
    "   Objetivo: criar um ponto de partida rÃ¡pido para testar estabilidade.\n",
    "   Vantagem: extremamente leve e previsÃ­vel.\n",
    "   LimitaÃ§Ã£o: ignora tendÃªncias (crescimento ou queda) e nÃ£o reage a sazonalidades.\n",
    "\n",
    "2. MÃ©dia MÃ³vel Ponderada\n",
    "   DescriÃ§Ã£o: mÃ©dia dos Ãºltimos 12 meses com pesos maiores para os meses mais recentes.\n",
    "   Objetivo: suavizar o histÃ³rico sem perder sensibilidade Ã  tendÃªncia recente.\n",
    "   Vantagem: melhora ligeiramente a resposta a movimentos recentes.\n",
    "   LimitaÃ§Ã£o: ainda nÃ£o reconhece padrÃµes anuais completos de sazonalidade.\n",
    "\n",
    "3. Sazonalidade Percentual HistÃ³rica\n",
    "   DescriÃ§Ã£o: calculava a participaÃ§Ã£o mÃ©dia de cada mÃªs no total anual.\n",
    "   Objetivo: reproduzir o comportamento sazonal real da empresa.\n",
    "   Vantagem: respeita picos e vales mensais do Ãºltimo ano completo.\n",
    "   LimitaÃ§Ã£o: dependente da qualidade do Ãºltimo ano â€” nÃ£o projeta volume total, apenas distribui.\n",
    "\n",
    "4. LightGBM\n",
    "   DescriÃ§Ã£o: modelo de machine learning (boosting de Ã¡rvores) aplicado sobre variÃ¡veis sazonais (seno/cosseno dos meses).\n",
    "   Objetivo: prever volumes mensais aprendendo padrÃµes nÃ£o lineares.\n",
    "   Vantagem: aprendizado rÃ¡pido e robusto em bases amplas.\n",
    "   LimitaÃ§Ã£o: exige ajuste fino e mais dados; em sÃ©ries curtas, tende a superajustar.\n",
    "\n",
    "5. RegressÃ£o Linear (nÃ­vel anual)\n",
    "   DescriÃ§Ã£o: ajusta uma reta sobre as vendas anuais (y = aÂ·x + b).\n",
    "   Objetivo: capturar tendÃªncias de crescimento ou queda sustentadas.\n",
    "   Vantagem: intuitivo e fÃ¡cil de justificar visualmente.\n",
    "   LimitaÃ§Ã£o: nÃ£o lida bem com oscilaÃ§Ãµes bruscas ou sÃ©ries curtas.\n",
    "\n",
    "6. Holt-Winters Aditivo\n",
    "   DescriÃ§Ã£o: modelo clÃ¡ssico de sÃ©ries temporais com nÃ­vel, tendÃªncia e sazonalidade (additive trend + seasonal).\n",
    "   Objetivo: gerar previsÃµes suaves mantendo padrÃ£o anual.\n",
    "   Vantagem: reconhecido e equilibrado entre suavidade e tendÃªncia.\n",
    "   LimitaÃ§Ã£o: pesado em grandes volumes e instÃ¡vel em sÃ©ries curtas.\n",
    "\n",
    "7. Ensemble EstatÃ­stico (fase intermediÃ¡ria)\n",
    "   DescriÃ§Ã£o: combinaÃ§Ã£o ponderada de modelos simples (RegressÃ£o + MÃ©dia + SuavizaÃ§Ã£o).\n",
    "   Objetivo: estabilizar volumes sem perder aparÃªncia estatÃ­stica.\n",
    "   Vantagem: resultados consistentes e realistas.\n",
    "   LimitaÃ§Ã£o: apresentava sempre o mesmo nome, sem variaÃ§Ã£o por empresa.\n",
    "      \n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "ðŸ“Š Modelos de PrevisÃ£o Aplicados (VersÃ£o Final)\n",
    "\n",
    "Este script aplica cinco modelos distintos de previsÃ£o de sÃ©ries temporais, compara o desempenho com base no RMSE\n",
    "(Root Mean Squared Error) e seleciona automaticamente o mais assertivo para gerar a projeÃ§Ã£o final.\n",
    "\n",
    "Modelos utilizados:\n",
    "\n",
    "1. RegressÃ£o Linear\n",
    "   - Captura: tendÃªncias lineares de longo prazo.\n",
    "   - Uso: quando o histÃ³rico mostra crescimento ou queda estÃ¡vel.\n",
    "   - ForÃ§a: fÃ¡cil de justificar e visualizar; ideal para projeÃ§Ãµes simples e diretas.\n",
    "\n",
    "2. Holt-Winters (SuavizaÃ§Ã£o Exponencial com sazonalidade multiplicativa)\n",
    "   - Captura: padrÃµes sazonais e tendÃªncia, com maior peso para os dados mais recentes.\n",
    "   - Uso: quando hÃ¡ sazonalidade clara e variaÃ§Ãµes proporcionais ao volume.\n",
    "   - ForÃ§a: modelo clÃ¡ssico, confiÃ¡vel e com excelente desempenho em sÃ©ries temporais mensais.\n",
    "\n",
    "3. ARIMA (AutoRegressive Integrated Moving Average)\n",
    "   - Captura: dependÃªncia temporal e ruÃ­do estatÃ­stico, sem necessidade de sazonalidade explÃ­cita.\n",
    "   - Uso: quando hÃ¡ padrÃ£o autoregressivo e estabilidade sem sazonalidade forte.\n",
    "   - ForÃ§a: modelo estatÃ­stico robusto, ideal para sÃ©ries estacionÃ¡rias ou suavizadas.\n",
    "\n",
    "4. Random Forest Regressor com variÃ¡veis sazonais\n",
    "   - Captura: relaÃ§Ãµes nÃ£o lineares e interaÃ§Ãµes entre tempo, mÃªs e ano.\n",
    "   - Uso: quando hÃ¡ sazonalidade, mas o padrÃ£o nÃ£o Ã© linear nem estÃ¡vel.\n",
    "   - ForÃ§a: flexÃ­vel, adaptÃ¡vel e resistente a ruÃ­dos; Ã³timo para sÃ©ries com comportamento irregular.\n",
    "\n",
    "5. Prophet (Facebook) com sazonalidade anual\n",
    "   - Captura: tendÃªncia, sazonalidade e feriados (se configurado).\n",
    "   - Uso: quando hÃ¡ sazonalidade anual bem definida e histÃ³rico suficiente.\n",
    "   - ForÃ§a: fÃ¡cil de ajustar, escalÃ¡vel e excelente para previsÃµes com mÃºltiplos componentes.\n",
    "\n",
    "O modelo com menor RMSE nos Ãºltimos 12 meses Ã© selecionado automaticamente para gerar a previsÃ£o final,\n",
    "que Ã© incorporada Ã  coluna 'VOL_VENDA_REAL' junto ao histÃ³rico, com marcaÃ§Ã£o do modelo escolhido.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ea3951",
   "metadata": {},
   "outputs": [],
   "source": [
    "parar_execucao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea423ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Iniciando processo de previsÃ£o estatÃ­stica...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     26\u001b[39m df_group = (\n\u001b[32m     27\u001b[39m     df_vendas_krona\n\u001b[32m     28\u001b[39m     .groupby([\u001b[33m\"\u001b[39m\u001b[33mCOD_PROD\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mPERIODO\u001b[39m\u001b[33m\"\u001b[39m], as_index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     29\u001b[39m     .agg(VOL_VENDA=(\u001b[33m\"\u001b[39m\u001b[33mVOL_VENDA\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msum\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     30\u001b[39m     .sort_values([\u001b[33m\"\u001b[39m\u001b[33mCOD_PROD\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mPERIODO\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     31\u001b[39m )\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# 2) CALENDÃRIO FUTURO (SEU) â€” SEM NORMALIZAR\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m     38\u001b[39m future_dates = pd.DatetimeIndex(\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[43mdf_periodo_previsao\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPERIODO_PROJECAO\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m.drop_duplicates().sort_values()\n\u001b[32m     40\u001b[39m )\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(future_dates) == \u001b[32m0\u001b[39m:\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mdf_periodo_previsao[\u001b[39m\u001b[33m'\u001b[39m\u001b[33mPERIODO_PROJECAO\u001b[39m\u001b[33m'\u001b[39m\u001b[33m] estÃ¡ vazio.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# PREVISAO ESTATISTICA â€” 5 MODELOS + MELHOR POR COD_PROD (SEM TRATAMENTO EXTRA)\n",
    "print(\"ðŸ”„ Iniciando processo de previsÃ£o estatÃ­stica...\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 0) CARREGAR df_vendas_krona DO PARQUET (para limpar memÃ³ria)\n",
    "# ============================================================\n",
    "\n",
    "df_vendas_krona = pd.read_parquet(pasta_staging_parquet / \"df_vendas_krona.parquet\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) AGRUPAMENTO PADRÃƒO (SEU)\n",
    "# ============================================================\n",
    "\n",
    "df_group = (\n",
    "    df_vendas_krona\n",
    "    .groupby([\"COD_PROD\", \"PERIODO\"], as_index=False)\n",
    "    .agg(VOL_VENDA=(\"VOL_VENDA\", \"sum\"))\n",
    "    .sort_values([\"COD_PROD\", \"PERIODO\"])\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) CALENDÃRIO FUTURO (SEU) â€” SEM NORMALIZAR\n",
    "# ============================================================\n",
    "\n",
    "future_dates = pd.DatetimeIndex(\n",
    "    df_periodo_previsao[\"PERIODO_PROJECAO\"].drop_duplicates().sort_values()\n",
    ")\n",
    "\n",
    "if len(future_dates) == 0:\n",
    "    raise ValueError(\"df_periodo_previsao['PERIODO_PROJECAO'] estÃ¡ vazio.\")\n",
    "\n",
    "primeiro_mes_previsao = future_dates.min()\n",
    "ultimo_mes_hist = primeiro_mes_previsao - pd.offsets.MonthBegin(1)\n",
    "\n",
    "df_hist_base = df_group[df_group[\"PERIODO\"] <= ultimo_mes_hist].copy()\n",
    "if df_hist_base.empty:\n",
    "    raise ValueError(\"HistÃ³rico vazio apÃ³s corte pelo calendÃ¡rio futuro.\")\n",
    "\n",
    "horizon = len(future_dates)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) MÃ‰TRICA (WAPE ou MAPE)\n",
    "# ============================================================\n",
    "\n",
    "METRICA_USADA = \"WAPE\"  # \"WAPE\" recomendado; se quiser \"MAPE\", troque aqui.\n",
    "\n",
    "def wape(y_true, y_pred) -> float:\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    denom = np.sum(np.abs(y_true))\n",
    "    if denom == 0:\n",
    "        return float(np.mean(np.abs(y_true - y_pred)))\n",
    "    return float(np.sum(np.abs(y_true - y_pred)) / denom)\n",
    "\n",
    "def safe_mape(y_true, y_pred) -> float:\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    mask = y_true != 0\n",
    "    if mask.sum() == 0:\n",
    "        return float(np.mean(np.abs(y_true - y_pred)))\n",
    "    return float(mean_absolute_percentage_error(y_true[mask], y_pred[mask]))\n",
    "\n",
    "def metric(y_true, y_pred) -> float:\n",
    "    return wape(y_true, y_pred) if METRICA_USADA == \"WAPE\" else safe_mape(y_true, y_pred)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) MODELOS (funÃ§Ãµes simples)\n",
    "# ============================================================\n",
    "\n",
    "def pred_hw(y_train, steps):\n",
    "    # tenta sazonal; se falhar, cai pra Holt trend\n",
    "    try:\n",
    "        m = ExponentialSmoothing(y_train, trend=\"add\", seasonal=\"multiplicative\", seasonal_periods=12).fit()\n",
    "        return np.maximum(m.forecast(steps), 0)\n",
    "    except Exception:\n",
    "        try:\n",
    "            m = ExponentialSmoothing(y_train, trend=\"add\", seasonal=\"additive\", seasonal_periods=12).fit()\n",
    "            return np.maximum(m.forecast(steps), 0)\n",
    "        except Exception:\n",
    "            m = ExponentialSmoothing(y_train, trend=\"add\", seasonal=None).fit()\n",
    "            return np.maximum(m.forecast(steps), 0)\n",
    "\n",
    "def pred_arima(y_train, steps):\n",
    "    m = ARIMA(y_train, order=(1,1,1)).fit()\n",
    "    return np.maximum(m.forecast(steps), 0)\n",
    "\n",
    "def pred_lr(y_train, steps):\n",
    "    y_train = np.asarray(y_train, dtype=float)\n",
    "    t = np.arange(len(y_train)).reshape(-1, 1)\n",
    "    lr = LinearRegression().fit(t, y_train)\n",
    "    t_future = np.arange(len(y_train), len(y_train) + steps).reshape(-1, 1)\n",
    "    return np.maximum(lr.predict(t_future), 0)\n",
    "\n",
    "def make_features_simple(y):\n",
    "    # features MINIMAS pra ML sem depender de painel:\n",
    "    # usa Ã­ndice de tempo + mÃªs (do prÃ³prio perÃ­odo)\n",
    "    return y\n",
    "\n",
    "def pred_rf(period_index_train, y_train, period_index_pred):\n",
    "    # features: time + mÃªs + ano\n",
    "    X_train = pd.DataFrame({\n",
    "        \"time\": np.arange(len(period_index_train)),\n",
    "        \"mes\": period_index_train.month,\n",
    "        \"ano\": period_index_train.year\n",
    "    })\n",
    "    rf = RandomForestRegressor(n_estimators=400, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    X_pred = pd.DataFrame({\n",
    "        \"time\": np.arange(len(period_index_train), len(period_index_train) + len(period_index_pred)),\n",
    "        \"mes\": period_index_pred.month,\n",
    "        \"ano\": period_index_pred.year\n",
    "    })\n",
    "    return np.maximum(rf.predict(X_pred), 0)\n",
    "\n",
    "def pred_gb(period_index_train, y_train, period_index_pred):\n",
    "    X_train = pd.DataFrame({\n",
    "        \"time\": np.arange(len(period_index_train)),\n",
    "        \"mes\": period_index_train.month,\n",
    "        \"ano\": period_index_train.year\n",
    "    })\n",
    "    gb = GradientBoostingRegressor(random_state=42)\n",
    "    gb.fit(X_train, y_train)\n",
    "\n",
    "    X_pred = pd.DataFrame({\n",
    "        \"time\": np.arange(len(period_index_train), len(period_index_train) + len(period_index_pred)),\n",
    "        \"mes\": period_index_pred.month,\n",
    "        \"ano\": period_index_pred.year\n",
    "    })\n",
    "    return np.maximum(gb.predict(X_pred), 0)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) ESCOLHER MELHOR MODELO POR COD_PROD (backtest) + prever futuro\n",
    "# ============================================================\n",
    "\n",
    "JANELA_VALIDACAO = 12\n",
    "\n",
    "registros = []\n",
    "\n",
    "for cod_prod, df_sku in df_hist_base.groupby(\"COD_PROD\"):\n",
    "    df_sku = df_sku.sort_values(\"PERIODO\")\n",
    "    y = df_sku[\"VOL_VENDA\"].values.astype(float)\n",
    "    idx = pd.DatetimeIndex(df_sku[\"PERIODO\"])\n",
    "\n",
    "    # histÃ³rico muito curto -> LR\n",
    "    if len(y) < 6:\n",
    "        fc = pred_lr(y, horizon)\n",
    "        best = \"LinearRegression_Fallback\"\n",
    "        for per, val in zip(future_dates, fc):\n",
    "            registros.append([cod_prod, per, float(val), best])\n",
    "        continue\n",
    "\n",
    "    J = min(JANELA_VALIDACAO, max(3, len(y)//3))\n",
    "    y_train, y_val = y[:-J], y[-J:]\n",
    "    idx_train, idx_val = idx[:-J], idx[-J:]\n",
    "\n",
    "    scores = {}\n",
    "\n",
    "    # 1) HW\n",
    "    try:\n",
    "        pred_val = pred_hw(y_train, J)\n",
    "        scores[\"HoltWinters\"] = metric(y_val, pred_val)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) ARIMA\n",
    "    try:\n",
    "        pred_val = pred_arima(y_train, J)\n",
    "        scores[\"ARIMA\"] = metric(y_val, pred_val)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 3) LR\n",
    "    try:\n",
    "        pred_val = pred_lr(y_train, J)\n",
    "        scores[\"LinearRegression\"] = metric(y_val, pred_val)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 4) RF (ML)\n",
    "    try:\n",
    "        pred_val = pred_rf(idx_train, y_train, idx_val)\n",
    "        scores[\"RandomForest\"] = metric(y_val, pred_val)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 5) GB (ML)\n",
    "    try:\n",
    "        pred_val = pred_gb(idx_train, y_train, idx_val)\n",
    "        scores[\"GradientBoosting\"] = metric(y_val, pred_val)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # escolher melhor e treinar no histÃ³rico completo\n",
    "    if not scores:\n",
    "        best = \"LinearRegression_Fallback\"\n",
    "        fc = pred_lr(y, horizon)\n",
    "    else:\n",
    "        best = min(scores.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "        if best == \"HoltWinters\":\n",
    "            fc = pred_hw(y, horizon)\n",
    "        elif best == \"ARIMA\":\n",
    "            fc = pred_arima(y, horizon)\n",
    "        elif best == \"LinearRegression\":\n",
    "            fc = pred_lr(y, horizon)\n",
    "        elif best == \"RandomForest\":\n",
    "            fc = pred_rf(idx, y, future_dates)\n",
    "        else:  # GradientBoosting\n",
    "            fc = pred_gb(idx, y, future_dates)\n",
    "\n",
    "    for per, val in zip(future_dates, fc):\n",
    "        registros.append([cod_prod, per, float(val), best])\n",
    "\n",
    "df_forecast = pd.DataFrame(registros, columns=[\"COD_PROD\", \"PERIODO\", \"VOL_VENDA_REAL\", \"MODELO_ESCOLHIDO\"])\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) SAÃDA FINAL (histÃ³rico + projeÃ§Ã£o)\n",
    "# ============================================================\n",
    "\n",
    "df_final_hist = df_hist_base.rename(columns={\"VOL_VENDA\": \"VOL_VENDA_REAL\"}).copy()\n",
    "df_final_hist[\"MODELO_ESCOLHIDO\"] = np.nan\n",
    "\n",
    "df_forecast_estatistico_krona = pd.concat([df_final_hist, df_forecast], ignore_index=True)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7) SALVAR CSV (EXATO COMO VOCÃŠ PEDIU)\n",
    "# ============================================================\n",
    "\n",
    "df_forecast_estatistico_krona.to_csv(\n",
    "    pasta_staging_parquet / \"df_forecast_estatistico_krona.csv\",\n",
    "    sep=\";\",\n",
    "    encoding=\"utf-8-sig\",\n",
    "    index=False,\n",
    "    decimal=\",\",\n",
    "    float_format=\"%.2f\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Finalizado e salvo: df_forecast_estatistico_krona.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788c5ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# DESAGREGAÃ‡ÃƒO INTELIGENTE BASEADA NO MIX DOS ÃšLTIMOS 6 MESES\n",
    "# ============================================================\n",
    "\n",
    "# 1) Definir data limite (6 meses atrÃ¡s do Ãºltimo mÃªs completo)\n",
    "data_limite = ultimo_mes_completo - pd.DateOffset(months=6)\n",
    "\n",
    "# 2) Filtrar histÃ³rico dos Ãºltimos 6 meses\n",
    "df_6m = df_vendas_krona[df_vendas_krona[\"PERIODO\"] > data_limite].copy()\n",
    "\n",
    "# 3) Somar volume por linha completa (todas as suas chaves originais)\n",
    "chaves = [\n",
    "    \"EMPRESA\",\n",
    "    \"COD_CLIENTE\",\n",
    "    \"NOME_CLIENTE\",\n",
    "    \"COD_GRUPO_CLIENTE\",\n",
    "    \"DESC_GRUPO_E_CLIENTE\",\n",
    "    \"COD_PROD\",\n",
    "    \"DESC_PRODUTO\",\n",
    "    \"FAMILIA\",\n",
    "    \"LINHA\",\n",
    "    \"REGIONAL\",\n",
    "    \"REGIONAL_GESTOR\"\n",
    "]\n",
    "\n",
    "df_mix = (\n",
    "    df_6m.groupby(chaves)[\"VOL_VENDA\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# 4) Total geral dos Ãºltimos 6 meses (somatÃ³rio de tudo)\n",
    "total_6m = df_mix[\"VOL_VENDA\"].sum()\n",
    "\n",
    "# 5) Criar PESO_MIX com base nos Ãºltimos 6 meses\n",
    "df_mix[\"PESO_MIX\"] = df_mix[\"VOL_VENDA\"] / total_6m\n",
    "\n",
    "# 6) Filtrar somente forecast futuro\n",
    "df_forecast_fut = df_forecast_estatistico_krona[\n",
    "    df_forecast_estatistico_krona[\"PERIODO\"] > ultimo_mes_completo\n",
    "].copy()\n",
    "\n",
    "# 7) Desagregar: multiplicar forecast mensal total pelo mix real\n",
    "df_forecast_desagregado = (\n",
    "    df_forecast_fut\n",
    "    .assign(key=1)\n",
    "    .merge(df_mix.assign(key=1), on=\"key\")\n",
    "    .drop(columns=\"key\")\n",
    ")\n",
    "\n",
    "df_forecast_desagregado[\"VOL_VENDA\"] = (\n",
    "    df_forecast_desagregado[\"VOL_VENDA_REAL\"] *\n",
    "    df_forecast_desagregado[\"PESO_MIX\"]\n",
    ")\n",
    "\n",
    "# 8) Selecionar colunas finais no mesmo layout do seu pipeline\n",
    "df_forecast_desagregado = df_forecast_desagregado[[\n",
    "    \"EMPRESA\",\n",
    "    \"COD_CLIENTE\",\n",
    "    \"NOME_CLIENTE\",\n",
    "    \"COD_GRUPO_CLIENTE\",\n",
    "    \"DESC_GRUPO_E_CLIENTE\",\n",
    "    \"COD_PROD\",\n",
    "    \"DESC_PRODUTO\",\n",
    "    \"FAMILIA\",\n",
    "    \"LINHA\",\n",
    "    \"REGIONAL\",\n",
    "    \"REGIONAL_GESTOR\",\n",
    "    \"PERIODO\",\n",
    "    \"VOL_VENDA\"\n",
    "]]\n",
    "\n",
    "# 9) Salvar\n",
    "df_forecast_desagregado.to_parquet(\n",
    "    pasta_staging_parquet / \"df_forecast_vendas_krona.parquet\",\n",
    "    index=False,\n",
    "    compression='snappy'\n",
    ")\n",
    "\n",
    "# FIXME\n",
    "del df_vendas_krona_hist_mes, df_final, df_forecast_final, df_lr, df_prophet, df_vendas_krona, df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a274c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Processamento EstatÃ­stico e Dados Forecast gerados com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Separar a df_forecast_vendas_krona em dois dataframes:\n",
    "# df_forecast_vendas_krona_CLIENTE: clientes que terÃ£o planejamento de demanda\n",
    "# df_forecast_vendas_krona_PRODUTO: produtos que terÃ£o planejamento de demanda\n",
    "\n",
    "df_forecast_vendas_krona = df_forecast_desagregado.copy()\n",
    "\n",
    "# Se lista_clientes_plan_demanda estiver vazio â†’ todos sÃ£o PRODUTO\n",
    "if lista_clientes_plan_demanda and len(lista_clientes_plan_demanda) > 0:\n",
    "    df_forecast_vendas_krona['NIVEL_PLAN_DEMANDA'] = np.where(\n",
    "        df_forecast_vendas_krona['COD_GRUPO_CLIENTE'].isin(lista_clientes_plan_demanda),\n",
    "        'CLIENTE',\n",
    "        'PRODUTO'\n",
    "    )\n",
    "else:\n",
    "    # Se nÃ£o existe cliente para plan. demanda â†’ tudo produto\n",
    "    df_forecast_vendas_krona['NIVEL_PLAN_DEMANDA'] = 'PRODUTO'\n",
    "\n",
    "\n",
    "# Separar os dataframes com cÃ³pia explÃ­cita\n",
    "df_forecast_vendas_krona_CLIENTE = df_forecast_vendas_krona[df_forecast_vendas_krona['NIVEL_PLAN_DEMANDA'] == 'CLIENTE'].copy()\n",
    "df_forecast_vendas_krona_PRODUTO = df_forecast_vendas_krona[df_forecast_vendas_krona['NIVEL_PLAN_DEMANDA'] == 'PRODUTO'].copy()\n",
    "\n",
    "# Eliminar coluna NIVEL_PLAN_DEMANDA\n",
    "df_forecast_vendas_krona_CLIENTE.drop(columns=['NIVEL_PLAN_DEMANDA'], inplace=True)\n",
    "df_forecast_vendas_krona_CLIENTE.reset_index(drop=True, inplace=True)\n",
    "df_forecast_vendas_krona_PRODUTO.drop(columns=['NIVEL_PLAN_DEMANDA'], inplace=True)\n",
    "df_forecast_vendas_krona_PRODUTO.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Eliminar colunas NOME_CLIENTE E DESC_GRUPO_E_CLIENTE\n",
    "df_forecast_vendas_krona_PRODUTO.drop(columns=['COD_CLIENTE', 'NOME_CLIENTE', 'COD_GRUPO_CLIENTE', 'DESC_GRUPO_E_CLIENTE'], inplace=True)\n",
    "\n",
    "# Sumarizar df_forecast_vendas_krona_PRODUTO por EMPRESA, COD_PROD, DESC_PRODUTO, FAMILIA, LINHA, REGIONAL, PERIODO\n",
    "df_forecast_vendas_krona_PRODUTO = df_forecast_vendas_krona_PRODUTO.groupby(\n",
    "    ['EMPRESA', 'COD_PROD', 'DESC_PRODUTO', 'FAMILIA', 'LINHA', 'REGIONAL', 'REGIONAL_GESTOR', 'PERIODO'],\n",
    "    as_index=False\n",
    ").agg({'VOL_VENDA': 'sum'}).reset_index(drop=True)\n",
    "\n",
    "# Trazer PESO_UNIT do parquet Dim_Produtos_Vendas_krona\n",
    "# Carregar parquet Dim_Produtos_Vendas_krona\n",
    "dim_produtos_vendas_krona = pd.read_parquet(\n",
    "    pasta_input_parquet / \"Dim_Produtos_Vendas_krona.parquet\"\n",
    ")\n",
    "\n",
    "# Normalizar maiuscula coluna Nom_Empresa\n",
    "def normalizar(texto):\n",
    "    if pd.isna(texto):\n",
    "        return ''\n",
    "    return ''.join(e for e in texto.upper() if e.isalnum())\n",
    "\n",
    "dim_produtos_vendas_krona[\"Nom_Empresa\"] = dim_produtos_vendas_krona[\"Nom_Empresa\"].str.upper()\n",
    "\n",
    "# Selecionar colunas necessÃ¡rias e remover duplicatas\n",
    "dim_produtos_vendas_krona = dim_produtos_vendas_krona[['Cod_Produto', 'Nom_Empresa', 'Num_Peso']].drop_duplicates(subset=['Cod_Produto', 'Nom_Empresa'])\n",
    "\n",
    "# Merge para trazer PESO_UNIT\n",
    "df_forecast_vendas_krona_PRODUTO = pd.merge(\n",
    "    df_forecast_vendas_krona_PRODUTO,\n",
    "    dim_produtos_vendas_krona,\n",
    "    how='left',\n",
    "    left_on=['COD_PROD', 'EMPRESA'],\n",
    "    right_on=['Cod_Produto', 'Nom_Empresa']\n",
    ")\n",
    "\n",
    "# Renomear coluna Num_Peso para PESO_UNIT\n",
    "df_forecast_vendas_krona_PRODUTO.rename(columns={'Num_Peso': 'PESO_UNIT'}, inplace=True)\n",
    "\n",
    "# Eliminar colunas desnecessÃ¡rias\n",
    "df_forecast_vendas_krona_PRODUTO.drop(columns=['Cod_Produto', 'Nom_Empresa'], inplace=True)\n",
    "\n",
    "# Gerar arquivos em PARQUET\n",
    "df_forecast_vendas_krona_PRODUTO.to_parquet(pasta_staging_parquet / 'df_forecast_vendas_krona_PRODUTO.parquet', index=False)\n",
    "df_forecast_vendas_krona_CLIENTE.to_parquet(pasta_staging_parquet / 'df_forecast_vendas_krona_CLIENTE.parquet', index=False)\n",
    "\n",
    "# ðŸ“¤ ExportaÃ§Ã£o de Dados Forecast para Planejamento Colaborativo\n",
    "# ðŸ“Š NÃ­vel de agregaÃ§Ã£o: REGIONAL_GESTOR, FAMILIA e PERIODO\n",
    "\n",
    "df_Forecast_PRODUTO = df_forecast_vendas_krona_PRODUTO.groupby(\n",
    "    ['REGIONAL_GESTOR', 'REGIONAL', 'FAMILIA', 'PERIODO'],\n",
    "    as_index=False\n",
    ").agg({'VOL_VENDA': 'sum'}).reset_index(drop=True)\n",
    "df_Forecast_PRODUTO.to_csv(\n",
    "    pasta_staging_parquet / 'FORECAST_KRONA_AGREGADO.csv',\n",
    "    sep=';',\n",
    "    encoding='utf-8-sig',\n",
    "    index=False,\n",
    "    decimal=',',\n",
    "    float_format=\"%.2f\"\n",
    ")\n",
    "\n",
    "# ðŸ“¤ ExportaÃ§Ã£o de Dados Forecast para Planejamento Colaborativo\n",
    "# ðŸ“Š NÃ­vel de agregaÃ§Ã£o: REGIONAL_GESTOR, COD_GRUPO_CLIENTE, DESC_GRUPO_E_CLIENTE, FAMILIA e PERIODO\n",
    "\n",
    "df_Forecast_CLIENTE = df_forecast_vendas_krona_CLIENTE.groupby(\n",
    "    ['REGIONAL_GESTOR', 'REGIONAL', 'COD_GRUPO_CLIENTE', 'DESC_GRUPO_E_CLIENTE', 'FAMILIA', 'PERIODO'],\n",
    "    as_index=False\n",
    ").agg({'VOL_VENDA': 'sum'}).reset_index(drop=True)\n",
    "df_Forecast_CLIENTE.to_csv(\n",
    "    pasta_staging_parquet / 'FORECAST_KRONA_CLIENTE.csv',\n",
    "    sep=';',\n",
    "    encoding='utf-8-sig',\n",
    "    index=False,\n",
    "    decimal=',',\n",
    "    float_format=\"%.2f\"\n",
    ")\n",
    "\n",
    "# FIXME\n",
    "del df_forecast_desagregado, dim_produtos_vendas_krona, df_forecast_vendas_krona_PRODUTO, df_forecast_vendas_krona_CLIENTE, lista_clientes_plan_demanda\n",
    "gc.collect()\n",
    "\n",
    "print(\"âœ… Processamento EstatÃ­stico e Dados Forecast gerados com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b935d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â±ï¸ Tempo total de processamento: 2 min 51.6 s\n",
      "ðŸŽ¯ Processo concluÃ­do com sucesso!\n"
     ]
    }
   ],
   "source": [
    "timer.finalizar()\n",
    "print(\"ðŸŽ¯ Processo concluÃ­do com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748e7423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pendencias\n",
    "# 1. Ao final do projeto, apagar os FIXME de validaÃ§Ã£o de dados que estÃ£o comentados no meio do programa\n",
    "# 2. Pendencia: Retirei dados de produtos lanÃ§amentos da lista de exclusÃ£o, pois preciso fazer previsÃ£o estatÃ­stica e considerar essa condiÃ§Ã£o conforme solicitaÃ§Ãµes da Anna no WORD\n",
    "# 3. Ver modelo de DesagregaÃ§Ã£o conforme implementao na VIQUA\n",
    "# 4. Ver com GPT sobre dimensionamento de VM para o projeto\n",
    "# 6. Verificar se todos as tabelas carregadas do arquivo KRONA_REGRAS estÃ£o sendo utilizadas no cÃ³digo final\n",
    "# 7. No cÃ³digo, ao importar os dados de vendas, se o ultimo mes for o mesmo mes de hoje, retirar esse mÃªs da base de vendas para evitar vazamento de dados\n",
    "# 8. Fazer um filtro por Cliente e Regional, e verificar porque existe cÃ³digo de cliente com duas regionais\n",
    "# 9. Corrigir endereÃ§os de pastas que foram alteradas no projeto\n",
    "# 10. Como resolver a duplicaÃ§Ã£o de clientes ou regionais, a princÃ­pio a duplicaÃ§Ã£o de chaves foi resolvida\n",
    "# 11. Criar visÃ£o resumo em KG e R$ conforme enviado modelo Anna\n",
    "# 12. Criar visÃ£o resumo, apÃ³s processar dados dos gerentes no pipeline, conforme modelo enviado por Anna. Esses quadros sÃ£o apresentados Ã  diretoria\n",
    "# 13. Criar formato para revisÃ£o do plano de demanda\n",
    "# 14. Criar um formato da exportaÃ§Ã£o de dados (para conferÃªncia conforme Anna enviarÃ¡ o formato do arquivo)\n",
    "\n",
    "# Ferramenta Excel\n",
    "# 1. Direcionar consumo das bases considerando a nova pasta BD_PLAN_COLAB_FERR_EXCEL dentro da pasta_staging_parquet\n",
    "\n",
    "# Pendencias resolvidas e aplicaveis as projetos VIQUA e LINEAR\n",
    "# 1. No cÃ³digo, ao importar os dados de vendas, se o ultimo mes for o mesmo mes de hoje, retirar esse mÃªs da base de vendas para evitar vazamento de dados\n",
    "\n",
    "# Comando criar Requirements.txt\n",
    "# pip freeze > \".\\00_SCRIPTS\\requirements.txt\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
